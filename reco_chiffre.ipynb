{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_data.data.shape\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    \"test\": DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders[\"train\"]):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders['train'].dataset)}] Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders[\"test\"]:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders[\"test\"].dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss}, Accuracy: {correct}/{len(loaders['test'].dataset)} ({100. * correct / len(loaders['test'].dataset)})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000] Loss: 2.3090527057647705\n",
      "Train Epoch: 1 [1000/60000] Loss: 2.2931160926818848\n",
      "Train Epoch: 1 [2000/60000] Loss: 2.309715986251831\n",
      "Train Epoch: 1 [3000/60000] Loss: 2.3121042251586914\n",
      "Train Epoch: 1 [4000/60000] Loss: 2.3011107444763184\n",
      "Train Epoch: 1 [5000/60000] Loss: 2.3152594566345215\n",
      "Train Epoch: 1 [6000/60000] Loss: 2.299689769744873\n",
      "Train Epoch: 1 [7000/60000] Loss: 2.301908493041992\n",
      "Train Epoch: 1 [8000/60000] Loss: 2.2894504070281982\n",
      "Train Epoch: 1 [9000/60000] Loss: 2.308558702468872\n",
      "Train Epoch: 1 [10000/60000] Loss: 2.315563201904297\n",
      "Train Epoch: 1 [11000/60000] Loss: 2.305633306503296\n",
      "Train Epoch: 1 [12000/60000] Loss: 2.3015384674072266\n",
      "Train Epoch: 1 [13000/60000] Loss: 2.294410467147827\n",
      "Train Epoch: 1 [14000/60000] Loss: 2.3060483932495117\n",
      "Train Epoch: 1 [15000/60000] Loss: 2.285630702972412\n",
      "Train Epoch: 1 [16000/60000] Loss: 2.307483434677124\n",
      "Train Epoch: 1 [17000/60000] Loss: 2.288752794265747\n",
      "Train Epoch: 1 [18000/60000] Loss: 2.3013978004455566\n",
      "Train Epoch: 1 [19000/60000] Loss: 2.299403190612793\n",
      "Train Epoch: 1 [20000/60000] Loss: 2.3062360286712646\n",
      "Train Epoch: 1 [21000/60000] Loss: 2.3095836639404297\n",
      "Train Epoch: 1 [22000/60000] Loss: 2.3121988773345947\n",
      "Train Epoch: 1 [23000/60000] Loss: 2.304518699645996\n",
      "Train Epoch: 1 [24000/60000] Loss: 2.3044042587280273\n",
      "Train Epoch: 1 [25000/60000] Loss: 2.293405055999756\n",
      "Train Epoch: 1 [26000/60000] Loss: 2.3160576820373535\n",
      "Train Epoch: 1 [27000/60000] Loss: 2.3072311878204346\n",
      "Train Epoch: 1 [28000/60000] Loss: 2.295947790145874\n",
      "Train Epoch: 1 [29000/60000] Loss: 2.3142271041870117\n",
      "Train Epoch: 1 [30000/60000] Loss: 2.299022912979126\n",
      "Train Epoch: 1 [31000/60000] Loss: 2.294518232345581\n",
      "Train Epoch: 1 [32000/60000] Loss: 2.3000564575195312\n",
      "Train Epoch: 1 [33000/60000] Loss: 2.3073596954345703\n",
      "Train Epoch: 1 [34000/60000] Loss: 2.292954444885254\n",
      "Train Epoch: 1 [35000/60000] Loss: 2.3122963905334473\n",
      "Train Epoch: 1 [36000/60000] Loss: 2.3088572025299072\n",
      "Train Epoch: 1 [37000/60000] Loss: 2.298309564590454\n",
      "Train Epoch: 1 [38000/60000] Loss: 2.3055973052978516\n",
      "Train Epoch: 1 [39000/60000] Loss: 2.306243658065796\n",
      "Train Epoch: 1 [40000/60000] Loss: 2.303386926651001\n",
      "Train Epoch: 1 [41000/60000] Loss: 2.320242166519165\n",
      "Train Epoch: 1 [42000/60000] Loss: 2.300847053527832\n",
      "Train Epoch: 1 [43000/60000] Loss: 2.306680679321289\n",
      "Train Epoch: 1 [44000/60000] Loss: 2.3093175888061523\n",
      "Train Epoch: 1 [45000/60000] Loss: 2.2991068363189697\n",
      "Train Epoch: 1 [46000/60000] Loss: 2.2927799224853516\n",
      "Train Epoch: 1 [47000/60000] Loss: 2.3061888217926025\n",
      "Train Epoch: 1 [48000/60000] Loss: 2.3178021907806396\n",
      "Train Epoch: 1 [49000/60000] Loss: 2.3022711277008057\n",
      "Train Epoch: 1 [50000/60000] Loss: 2.305898427963257\n",
      "Train Epoch: 1 [51000/60000] Loss: 2.3054521083831787\n",
      "Train Epoch: 1 [52000/60000] Loss: 2.306047201156616\n",
      "Train Epoch: 1 [53000/60000] Loss: 2.3053781986236572\n",
      "Train Epoch: 1 [54000/60000] Loss: 2.305515766143799\n",
      "Train Epoch: 1 [55000/60000] Loss: 2.3020596504211426\n",
      "Train Epoch: 1 [56000/60000] Loss: 2.2940003871917725\n",
      "Train Epoch: 1 [57000/60000] Loss: 2.314176559448242\n",
      "Train Epoch: 1 [58000/60000] Loss: 2.3031554222106934\n",
      "Train Epoch: 1 [59000/60000] Loss: 2.2966690063476562\n",
      "\n",
      "Test set: Average loss: 0.023000377774238585, Accuracy: 692/10000 (6.92)\n",
      "\n",
      "Train Epoch: 2 [0/60000] Loss: 2.3067829608917236\n",
      "Train Epoch: 2 [1000/60000] Loss: 2.2994353771209717\n",
      "Train Epoch: 2 [2000/60000] Loss: 2.304875373840332\n",
      "Train Epoch: 2 [3000/60000] Loss: 2.315479040145874\n",
      "Train Epoch: 2 [4000/60000] Loss: 2.3046905994415283\n",
      "Train Epoch: 2 [5000/60000] Loss: 2.2986536026000977\n",
      "Train Epoch: 2 [6000/60000] Loss: 2.294501304626465\n",
      "Train Epoch: 2 [7000/60000] Loss: 2.31306529045105\n",
      "Train Epoch: 2 [8000/60000] Loss: 2.2984817028045654\n",
      "Train Epoch: 2 [9000/60000] Loss: 2.305616855621338\n",
      "Train Epoch: 2 [10000/60000] Loss: 2.311312198638916\n",
      "Train Epoch: 2 [11000/60000] Loss: 2.2949559688568115\n",
      "Train Epoch: 2 [12000/60000] Loss: 2.3039090633392334\n",
      "Train Epoch: 2 [13000/60000] Loss: 2.2848803997039795\n",
      "Train Epoch: 2 [14000/60000] Loss: 2.2992286682128906\n",
      "Train Epoch: 2 [15000/60000] Loss: 2.2911717891693115\n",
      "Train Epoch: 2 [16000/60000] Loss: 2.283052682876587\n",
      "Train Epoch: 2 [17000/60000] Loss: 2.3035967350006104\n",
      "Train Epoch: 2 [18000/60000] Loss: 2.2977659702301025\n",
      "Train Epoch: 2 [19000/60000] Loss: 2.2926487922668457\n",
      "Train Epoch: 2 [20000/60000] Loss: 2.294210433959961\n",
      "Train Epoch: 2 [21000/60000] Loss: 2.311370611190796\n",
      "Train Epoch: 2 [22000/60000] Loss: 2.2966394424438477\n",
      "Train Epoch: 2 [23000/60000] Loss: 2.30181884765625\n",
      "Train Epoch: 2 [24000/60000] Loss: 2.3057329654693604\n",
      "Train Epoch: 2 [25000/60000] Loss: 2.3052713871002197\n",
      "Train Epoch: 2 [26000/60000] Loss: 2.3026418685913086\n",
      "Train Epoch: 2 [27000/60000] Loss: 2.302914619445801\n",
      "Train Epoch: 2 [28000/60000] Loss: 2.2950563430786133\n",
      "Train Epoch: 2 [29000/60000] Loss: 2.310911178588867\n",
      "Train Epoch: 2 [30000/60000] Loss: 2.307396173477173\n",
      "Train Epoch: 2 [31000/60000] Loss: 2.300121307373047\n",
      "Train Epoch: 2 [32000/60000] Loss: 2.2886910438537598\n",
      "Train Epoch: 2 [33000/60000] Loss: 2.3030893802642822\n",
      "Train Epoch: 2 [34000/60000] Loss: 2.2990736961364746\n",
      "Train Epoch: 2 [35000/60000] Loss: 2.3041436672210693\n",
      "Train Epoch: 2 [36000/60000] Loss: 2.297764778137207\n",
      "Train Epoch: 2 [37000/60000] Loss: 2.3052210807800293\n",
      "Train Epoch: 2 [38000/60000] Loss: 2.291893482208252\n",
      "Train Epoch: 2 [39000/60000] Loss: 2.3093173503875732\n",
      "Train Epoch: 2 [40000/60000] Loss: 2.3013038635253906\n",
      "Train Epoch: 2 [41000/60000] Loss: 2.303762674331665\n",
      "Train Epoch: 2 [42000/60000] Loss: 2.297435998916626\n",
      "Train Epoch: 2 [43000/60000] Loss: 2.292241096496582\n",
      "Train Epoch: 2 [44000/60000] Loss: 2.2830147743225098\n",
      "Train Epoch: 2 [45000/60000] Loss: 2.3012781143188477\n",
      "Train Epoch: 2 [46000/60000] Loss: 2.2965869903564453\n",
      "Train Epoch: 2 [47000/60000] Loss: 2.3042380809783936\n",
      "Train Epoch: 2 [48000/60000] Loss: 2.291663885116577\n",
      "Train Epoch: 2 [49000/60000] Loss: 2.302222967147827\n",
      "Train Epoch: 2 [50000/60000] Loss: 2.2959837913513184\n",
      "Train Epoch: 2 [51000/60000] Loss: 2.2947254180908203\n",
      "Train Epoch: 2 [52000/60000] Loss: 2.2897629737854004\n",
      "Train Epoch: 2 [53000/60000] Loss: 2.2878291606903076\n",
      "Train Epoch: 2 [54000/60000] Loss: 2.2972846031188965\n",
      "Train Epoch: 2 [55000/60000] Loss: 2.288532018661499\n",
      "Train Epoch: 2 [56000/60000] Loss: 2.294618844985962\n",
      "Train Epoch: 2 [57000/60000] Loss: 2.2992665767669678\n",
      "Train Epoch: 2 [58000/60000] Loss: 2.3045778274536133\n",
      "Train Epoch: 2 [59000/60000] Loss: 2.291045904159546\n",
      "\n",
      "Test set: Average loss: 0.022957612347602845, Accuracy: 1029/10000 (10.29)\n",
      "\n",
      "Train Epoch: 3 [0/60000] Loss: 2.3007919788360596\n",
      "Train Epoch: 3 [1000/60000] Loss: 2.2964563369750977\n",
      "Train Epoch: 3 [2000/60000] Loss: 2.290447473526001\n",
      "Train Epoch: 3 [3000/60000] Loss: 2.2878897190093994\n",
      "Train Epoch: 3 [4000/60000] Loss: 2.287903308868408\n",
      "Train Epoch: 3 [5000/60000] Loss: 2.305069923400879\n",
      "Train Epoch: 3 [6000/60000] Loss: 2.3012938499450684\n",
      "Train Epoch: 3 [7000/60000] Loss: 2.2964017391204834\n",
      "Train Epoch: 3 [8000/60000] Loss: 2.3029415607452393\n",
      "Train Epoch: 3 [9000/60000] Loss: 2.296210527420044\n",
      "Train Epoch: 3 [10000/60000] Loss: 2.2976229190826416\n",
      "Train Epoch: 3 [11000/60000] Loss: 2.2952699661254883\n",
      "Train Epoch: 3 [12000/60000] Loss: 2.2966411113739014\n",
      "Train Epoch: 3 [13000/60000] Loss: 2.2953813076019287\n",
      "Train Epoch: 3 [14000/60000] Loss: 2.297708511352539\n",
      "Train Epoch: 3 [15000/60000] Loss: 2.2970993518829346\n",
      "Train Epoch: 3 [16000/60000] Loss: 2.2926833629608154\n",
      "Train Epoch: 3 [17000/60000] Loss: 2.2902817726135254\n",
      "Train Epoch: 3 [18000/60000] Loss: 2.3040900230407715\n",
      "Train Epoch: 3 [19000/60000] Loss: 2.2881784439086914\n",
      "Train Epoch: 3 [20000/60000] Loss: 2.3060734272003174\n",
      "Train Epoch: 3 [21000/60000] Loss: 2.2810957431793213\n",
      "Train Epoch: 3 [22000/60000] Loss: 2.2917912006378174\n",
      "Train Epoch: 3 [23000/60000] Loss: 2.2987747192382812\n",
      "Train Epoch: 3 [24000/60000] Loss: 2.2930991649627686\n",
      "Train Epoch: 3 [25000/60000] Loss: 2.3004631996154785\n",
      "Train Epoch: 3 [26000/60000] Loss: 2.3004331588745117\n",
      "Train Epoch: 3 [27000/60000] Loss: 2.307823896408081\n",
      "Train Epoch: 3 [28000/60000] Loss: 2.2993507385253906\n",
      "Train Epoch: 3 [29000/60000] Loss: 2.2914905548095703\n",
      "Train Epoch: 3 [30000/60000] Loss: 2.2999167442321777\n",
      "Train Epoch: 3 [31000/60000] Loss: 2.3029160499572754\n",
      "Train Epoch: 3 [32000/60000] Loss: 2.301736354827881\n",
      "Train Epoch: 3 [33000/60000] Loss: 2.29628849029541\n",
      "Train Epoch: 3 [34000/60000] Loss: 2.2800140380859375\n",
      "Train Epoch: 3 [35000/60000] Loss: 2.298076868057251\n",
      "Train Epoch: 3 [36000/60000] Loss: 2.281062364578247\n",
      "Train Epoch: 3 [37000/60000] Loss: 2.29715895652771\n",
      "Train Epoch: 3 [38000/60000] Loss: 2.2943074703216553\n",
      "Train Epoch: 3 [39000/60000] Loss: 2.304670810699463\n",
      "Train Epoch: 3 [40000/60000] Loss: 2.2838456630706787\n",
      "Train Epoch: 3 [41000/60000] Loss: 2.295830726623535\n",
      "Train Epoch: 3 [42000/60000] Loss: 2.298790693283081\n",
      "Train Epoch: 3 [43000/60000] Loss: 2.2925779819488525\n",
      "Train Epoch: 3 [44000/60000] Loss: 2.2966365814208984\n",
      "Train Epoch: 3 [45000/60000] Loss: 2.302525281906128\n",
      "Train Epoch: 3 [46000/60000] Loss: 2.2961902618408203\n",
      "Train Epoch: 3 [47000/60000] Loss: 2.289259672164917\n",
      "Train Epoch: 3 [48000/60000] Loss: 2.304514169692993\n",
      "Train Epoch: 3 [49000/60000] Loss: 2.2935543060302734\n",
      "Train Epoch: 3 [50000/60000] Loss: 2.2904574871063232\n",
      "Train Epoch: 3 [51000/60000] Loss: 2.294832229614258\n",
      "Train Epoch: 3 [52000/60000] Loss: 2.308350086212158\n",
      "Train Epoch: 3 [53000/60000] Loss: 2.302257776260376\n",
      "Train Epoch: 3 [54000/60000] Loss: 2.2925076484680176\n",
      "Train Epoch: 3 [55000/60000] Loss: 2.3007919788360596\n",
      "Train Epoch: 3 [56000/60000] Loss: 2.299027442932129\n",
      "Train Epoch: 3 [57000/60000] Loss: 2.287864923477173\n",
      "Train Epoch: 3 [58000/60000] Loss: 2.2957146167755127\n",
      "Train Epoch: 3 [59000/60000] Loss: 2.2813165187835693\n",
      "\n",
      "Test set: Average loss: 0.022913349533081055, Accuracy: 1718/10000 (17.18)\n",
      "\n",
      "Train Epoch: 4 [0/60000] Loss: 2.288339614868164\n",
      "Train Epoch: 4 [1000/60000] Loss: 2.2878780364990234\n",
      "Train Epoch: 4 [2000/60000] Loss: 2.3023667335510254\n",
      "Train Epoch: 4 [3000/60000] Loss: 2.297893762588501\n",
      "Train Epoch: 4 [4000/60000] Loss: 2.292336940765381\n",
      "Train Epoch: 4 [5000/60000] Loss: 2.2979021072387695\n",
      "Train Epoch: 4 [6000/60000] Loss: 2.2995119094848633\n",
      "Train Epoch: 4 [7000/60000] Loss: 2.296347141265869\n",
      "Train Epoch: 4 [8000/60000] Loss: 2.2949957847595215\n",
      "Train Epoch: 4 [9000/60000] Loss: 2.2895920276641846\n",
      "Train Epoch: 4 [10000/60000] Loss: 2.285736560821533\n",
      "Train Epoch: 4 [11000/60000] Loss: 2.297085762023926\n",
      "Train Epoch: 4 [12000/60000] Loss: 2.300699234008789\n",
      "Train Epoch: 4 [13000/60000] Loss: 2.2899529933929443\n",
      "Train Epoch: 4 [14000/60000] Loss: 2.300459623336792\n",
      "Train Epoch: 4 [15000/60000] Loss: 2.2969722747802734\n",
      "Train Epoch: 4 [16000/60000] Loss: 2.297637462615967\n",
      "Train Epoch: 4 [17000/60000] Loss: 2.3022539615631104\n",
      "Train Epoch: 4 [18000/60000] Loss: 2.2829761505126953\n",
      "Train Epoch: 4 [19000/60000] Loss: 2.2898244857788086\n",
      "Train Epoch: 4 [20000/60000] Loss: 2.2959601879119873\n",
      "Train Epoch: 4 [21000/60000] Loss: 2.2981154918670654\n",
      "Train Epoch: 4 [22000/60000] Loss: 2.299863815307617\n",
      "Train Epoch: 4 [23000/60000] Loss: 2.283656120300293\n",
      "Train Epoch: 4 [24000/60000] Loss: 2.297112226486206\n",
      "Train Epoch: 4 [25000/60000] Loss: 2.2992827892303467\n",
      "Train Epoch: 4 [26000/60000] Loss: 2.2884953022003174\n",
      "Train Epoch: 4 [27000/60000] Loss: 2.291548728942871\n",
      "Train Epoch: 4 [28000/60000] Loss: 2.2790367603302\n",
      "Train Epoch: 4 [29000/60000] Loss: 2.2875568866729736\n",
      "Train Epoch: 4 [30000/60000] Loss: 2.280006170272827\n",
      "Train Epoch: 4 [31000/60000] Loss: 2.297010898590088\n",
      "Train Epoch: 4 [32000/60000] Loss: 2.28822660446167\n",
      "Train Epoch: 4 [33000/60000] Loss: 2.2861990928649902\n",
      "Train Epoch: 4 [34000/60000] Loss: 2.286494255065918\n",
      "Train Epoch: 4 [35000/60000] Loss: 2.285754919052124\n",
      "Train Epoch: 4 [36000/60000] Loss: 2.2895760536193848\n",
      "Train Epoch: 4 [37000/60000] Loss: 2.287426233291626\n",
      "Train Epoch: 4 [38000/60000] Loss: 2.283083438873291\n",
      "Train Epoch: 4 [39000/60000] Loss: 2.2928333282470703\n",
      "Train Epoch: 4 [40000/60000] Loss: 2.290673017501831\n",
      "Train Epoch: 4 [41000/60000] Loss: 2.2822153568267822\n",
      "Train Epoch: 4 [42000/60000] Loss: 2.2895843982696533\n",
      "Train Epoch: 4 [43000/60000] Loss: 2.296452522277832\n",
      "Train Epoch: 4 [44000/60000] Loss: 2.287703037261963\n",
      "Train Epoch: 4 [45000/60000] Loss: 2.2935311794281006\n",
      "Train Epoch: 4 [46000/60000] Loss: 2.284228563308716\n",
      "Train Epoch: 4 [47000/60000] Loss: 2.2942028045654297\n",
      "Train Epoch: 4 [48000/60000] Loss: 2.2836356163024902\n",
      "Train Epoch: 4 [49000/60000] Loss: 2.3125481605529785\n",
      "Train Epoch: 4 [50000/60000] Loss: 2.2903950214385986\n",
      "Train Epoch: 4 [51000/60000] Loss: 2.2892367839813232\n",
      "Train Epoch: 4 [52000/60000] Loss: 2.287971019744873\n",
      "Train Epoch: 4 [53000/60000] Loss: 2.2882394790649414\n",
      "Train Epoch: 4 [54000/60000] Loss: 2.2867488861083984\n",
      "Train Epoch: 4 [55000/60000] Loss: 2.296764850616455\n",
      "Train Epoch: 4 [56000/60000] Loss: 2.2957053184509277\n",
      "Train Epoch: 4 [57000/60000] Loss: 2.298823833465576\n",
      "Train Epoch: 4 [58000/60000] Loss: 2.2740414142608643\n",
      "Train Epoch: 4 [59000/60000] Loss: 2.290703058242798\n",
      "\n",
      "Test set: Average loss: 0.022859302115440367, Accuracy: 2295/10000 (22.95)\n",
      "\n",
      "Train Epoch: 5 [0/60000] Loss: 2.2818381786346436\n",
      "Train Epoch: 5 [1000/60000] Loss: 2.291520357131958\n",
      "Train Epoch: 5 [2000/60000] Loss: 2.2766127586364746\n",
      "Train Epoch: 5 [3000/60000] Loss: 2.2830190658569336\n",
      "Train Epoch: 5 [4000/60000] Loss: 2.2832677364349365\n",
      "Train Epoch: 5 [5000/60000] Loss: 2.2869198322296143\n",
      "Train Epoch: 5 [6000/60000] Loss: 2.2863636016845703\n",
      "Train Epoch: 5 [7000/60000] Loss: 2.2940833568573\n",
      "Train Epoch: 5 [8000/60000] Loss: 2.2861225605010986\n",
      "Train Epoch: 5 [9000/60000] Loss: 2.2898073196411133\n",
      "Train Epoch: 5 [10000/60000] Loss: 2.295302629470825\n",
      "Train Epoch: 5 [11000/60000] Loss: 2.2894132137298584\n",
      "Train Epoch: 5 [12000/60000] Loss: 2.289674758911133\n",
      "Train Epoch: 5 [13000/60000] Loss: 2.2941925525665283\n",
      "Train Epoch: 5 [14000/60000] Loss: 2.2845041751861572\n",
      "Train Epoch: 5 [15000/60000] Loss: 2.2942724227905273\n",
      "Train Epoch: 5 [16000/60000] Loss: 2.292459011077881\n",
      "Train Epoch: 5 [17000/60000] Loss: 2.2952489852905273\n",
      "Train Epoch: 5 [18000/60000] Loss: 2.2994577884674072\n",
      "Train Epoch: 5 [19000/60000] Loss: 2.299259662628174\n",
      "Train Epoch: 5 [20000/60000] Loss: 2.2922303676605225\n",
      "Train Epoch: 5 [21000/60000] Loss: 2.287445306777954\n",
      "Train Epoch: 5 [22000/60000] Loss: 2.292109727859497\n",
      "Train Epoch: 5 [23000/60000] Loss: 2.2930514812469482\n",
      "Train Epoch: 5 [24000/60000] Loss: 2.2978017330169678\n",
      "Train Epoch: 5 [25000/60000] Loss: 2.295872926712036\n",
      "Train Epoch: 5 [26000/60000] Loss: 2.2870006561279297\n",
      "Train Epoch: 5 [27000/60000] Loss: 2.2896909713745117\n",
      "Train Epoch: 5 [28000/60000] Loss: 2.292933702468872\n",
      "Train Epoch: 5 [29000/60000] Loss: 2.2966623306274414\n",
      "Train Epoch: 5 [30000/60000] Loss: 2.3037562370300293\n",
      "Train Epoch: 5 [31000/60000] Loss: 2.2843384742736816\n",
      "Train Epoch: 5 [32000/60000] Loss: 2.283921718597412\n",
      "Train Epoch: 5 [33000/60000] Loss: 2.291170597076416\n",
      "Train Epoch: 5 [34000/60000] Loss: 2.282960891723633\n",
      "Train Epoch: 5 [35000/60000] Loss: 2.2870774269104004\n",
      "Train Epoch: 5 [36000/60000] Loss: 2.2945806980133057\n",
      "Train Epoch: 5 [37000/60000] Loss: 2.280733346939087\n",
      "Train Epoch: 5 [38000/60000] Loss: 2.2872397899627686\n",
      "Train Epoch: 5 [39000/60000] Loss: 2.2932708263397217\n",
      "Train Epoch: 5 [40000/60000] Loss: 2.285625696182251\n",
      "Train Epoch: 5 [41000/60000] Loss: 2.2824347019195557\n",
      "Train Epoch: 5 [42000/60000] Loss: 2.2857375144958496\n",
      "Train Epoch: 5 [43000/60000] Loss: 2.2861032485961914\n",
      "Train Epoch: 5 [44000/60000] Loss: 2.290053129196167\n",
      "Train Epoch: 5 [45000/60000] Loss: 2.2869033813476562\n",
      "Train Epoch: 5 [46000/60000] Loss: 2.2869462966918945\n",
      "Train Epoch: 5 [47000/60000] Loss: 2.290426254272461\n",
      "Train Epoch: 5 [48000/60000] Loss: 2.2777938842773438\n",
      "Train Epoch: 5 [49000/60000] Loss: 2.279388666152954\n",
      "Train Epoch: 5 [50000/60000] Loss: 2.287057399749756\n",
      "Train Epoch: 5 [51000/60000] Loss: 2.2755088806152344\n",
      "Train Epoch: 5 [52000/60000] Loss: 2.2808167934417725\n",
      "Train Epoch: 5 [53000/60000] Loss: 2.2780873775482178\n",
      "Train Epoch: 5 [54000/60000] Loss: 2.2907166481018066\n",
      "Train Epoch: 5 [55000/60000] Loss: 2.281787395477295\n",
      "Train Epoch: 5 [56000/60000] Loss: 2.2758820056915283\n",
      "Train Epoch: 5 [57000/60000] Loss: 2.292003870010376\n",
      "Train Epoch: 5 [58000/60000] Loss: 2.293466806411743\n",
      "Train Epoch: 5 [59000/60000] Loss: 2.298551559448242\n",
      "\n",
      "Test set: Average loss: 0.022788886833190917, Accuracy: 2787/10000 (27.87)\n",
      "\n",
      "Train Epoch: 6 [0/60000] Loss: 2.265373945236206\n",
      "Train Epoch: 6 [1000/60000] Loss: 2.2775163650512695\n",
      "Train Epoch: 6 [2000/60000] Loss: 2.286675453186035\n",
      "Train Epoch: 6 [3000/60000] Loss: 2.285043478012085\n",
      "Train Epoch: 6 [4000/60000] Loss: 2.286336898803711\n",
      "Train Epoch: 6 [5000/60000] Loss: 2.2786900997161865\n",
      "Train Epoch: 6 [6000/60000] Loss: 2.273843765258789\n",
      "Train Epoch: 6 [7000/60000] Loss: 2.2983646392822266\n",
      "Train Epoch: 6 [8000/60000] Loss: 2.287116289138794\n",
      "Train Epoch: 6 [9000/60000] Loss: 2.2932820320129395\n",
      "Train Epoch: 6 [10000/60000] Loss: 2.292330503463745\n",
      "Train Epoch: 6 [11000/60000] Loss: 2.28474760055542\n",
      "Train Epoch: 6 [12000/60000] Loss: 2.2818491458892822\n",
      "Train Epoch: 6 [13000/60000] Loss: 2.27883243560791\n",
      "Train Epoch: 6 [14000/60000] Loss: 2.2829816341400146\n",
      "Train Epoch: 6 [15000/60000] Loss: 2.282214403152466\n",
      "Train Epoch: 6 [16000/60000] Loss: 2.2782769203186035\n",
      "Train Epoch: 6 [17000/60000] Loss: 2.27652645111084\n",
      "Train Epoch: 6 [18000/60000] Loss: 2.2780885696411133\n",
      "Train Epoch: 6 [19000/60000] Loss: 2.2765114307403564\n",
      "Train Epoch: 6 [20000/60000] Loss: 2.283572196960449\n",
      "Train Epoch: 6 [21000/60000] Loss: 2.2856388092041016\n",
      "Train Epoch: 6 [22000/60000] Loss: 2.280367851257324\n",
      "Train Epoch: 6 [23000/60000] Loss: 2.281008243560791\n",
      "Train Epoch: 6 [24000/60000] Loss: 2.2804219722747803\n",
      "Train Epoch: 6 [25000/60000] Loss: 2.2784297466278076\n",
      "Train Epoch: 6 [26000/60000] Loss: 2.2821364402770996\n",
      "Train Epoch: 6 [27000/60000] Loss: 2.271312952041626\n",
      "Train Epoch: 6 [28000/60000] Loss: 2.2882497310638428\n",
      "Train Epoch: 6 [29000/60000] Loss: 2.2784833908081055\n",
      "Train Epoch: 6 [30000/60000] Loss: 2.2943460941314697\n",
      "Train Epoch: 6 [31000/60000] Loss: 2.284630537033081\n",
      "Train Epoch: 6 [32000/60000] Loss: 2.270885944366455\n",
      "Train Epoch: 6 [33000/60000] Loss: 2.282430410385132\n",
      "Train Epoch: 6 [34000/60000] Loss: 2.2759978771209717\n",
      "Train Epoch: 6 [35000/60000] Loss: 2.274243116378784\n",
      "Train Epoch: 6 [36000/60000] Loss: 2.2904434204101562\n",
      "Train Epoch: 6 [37000/60000] Loss: 2.2871017456054688\n",
      "Train Epoch: 6 [38000/60000] Loss: 2.2934987545013428\n",
      "Train Epoch: 6 [39000/60000] Loss: 2.270540714263916\n",
      "Train Epoch: 6 [40000/60000] Loss: 2.271341323852539\n",
      "Train Epoch: 6 [41000/60000] Loss: 2.2814977169036865\n",
      "Train Epoch: 6 [42000/60000] Loss: 2.2741024494171143\n",
      "Train Epoch: 6 [43000/60000] Loss: 2.27913761138916\n",
      "Train Epoch: 6 [44000/60000] Loss: 2.277575731277466\n",
      "Train Epoch: 6 [45000/60000] Loss: 2.274702548980713\n",
      "Train Epoch: 6 [46000/60000] Loss: 2.2742063999176025\n",
      "Train Epoch: 6 [47000/60000] Loss: 2.272393226623535\n",
      "Train Epoch: 6 [48000/60000] Loss: 2.2617528438568115\n",
      "Train Epoch: 6 [49000/60000] Loss: 2.284912347793579\n",
      "Train Epoch: 6 [50000/60000] Loss: 2.282153844833374\n",
      "Train Epoch: 6 [51000/60000] Loss: 2.2784924507141113\n",
      "Train Epoch: 6 [52000/60000] Loss: 2.2859303951263428\n",
      "Train Epoch: 6 [53000/60000] Loss: 2.2700283527374268\n",
      "Train Epoch: 6 [54000/60000] Loss: 2.2925117015838623\n",
      "Train Epoch: 6 [55000/60000] Loss: 2.2682743072509766\n",
      "Train Epoch: 6 [56000/60000] Loss: 2.276484966278076\n",
      "Train Epoch: 6 [57000/60000] Loss: 2.270510196685791\n",
      "Train Epoch: 6 [58000/60000] Loss: 2.264002799987793\n",
      "Train Epoch: 6 [59000/60000] Loss: 2.273261547088623\n",
      "\n",
      "Test set: Average loss: 0.02268884792327881, Accuracy: 3195/10000 (31.95)\n",
      "\n",
      "Train Epoch: 7 [0/60000] Loss: 2.2868175506591797\n",
      "Train Epoch: 7 [1000/60000] Loss: 2.2723426818847656\n",
      "Train Epoch: 7 [2000/60000] Loss: 2.2752461433410645\n",
      "Train Epoch: 7 [3000/60000] Loss: 2.2695255279541016\n",
      "Train Epoch: 7 [4000/60000] Loss: 2.280602216720581\n",
      "Train Epoch: 7 [5000/60000] Loss: 2.264784336090088\n",
      "Train Epoch: 7 [6000/60000] Loss: 2.281998634338379\n",
      "Train Epoch: 7 [7000/60000] Loss: 2.281547784805298\n",
      "Train Epoch: 7 [8000/60000] Loss: 2.265934705734253\n",
      "Train Epoch: 7 [9000/60000] Loss: 2.279449224472046\n",
      "Train Epoch: 7 [10000/60000] Loss: 2.2828798294067383\n",
      "Train Epoch: 7 [11000/60000] Loss: 2.2787528038024902\n",
      "Train Epoch: 7 [12000/60000] Loss: 2.2697432041168213\n",
      "Train Epoch: 7 [13000/60000] Loss: 2.2756762504577637\n",
      "Train Epoch: 7 [14000/60000] Loss: 2.264023542404175\n",
      "Train Epoch: 7 [15000/60000] Loss: 2.267576217651367\n",
      "Train Epoch: 7 [16000/60000] Loss: 2.265336036682129\n",
      "Train Epoch: 7 [17000/60000] Loss: 2.2779600620269775\n",
      "Train Epoch: 7 [18000/60000] Loss: 2.262298583984375\n",
      "Train Epoch: 7 [19000/60000] Loss: 2.2708933353424072\n",
      "Train Epoch: 7 [20000/60000] Loss: 2.26450252532959\n",
      "Train Epoch: 7 [21000/60000] Loss: 2.2570390701293945\n",
      "Train Epoch: 7 [22000/60000] Loss: 2.265887975692749\n",
      "Train Epoch: 7 [23000/60000] Loss: 2.273617744445801\n",
      "Train Epoch: 7 [24000/60000] Loss: 2.275693655014038\n",
      "Train Epoch: 7 [25000/60000] Loss: 2.270033836364746\n",
      "Train Epoch: 7 [26000/60000] Loss: 2.2712035179138184\n",
      "Train Epoch: 7 [27000/60000] Loss: 2.2701241970062256\n",
      "Train Epoch: 7 [28000/60000] Loss: 2.2521486282348633\n",
      "Train Epoch: 7 [29000/60000] Loss: 2.286020278930664\n",
      "Train Epoch: 7 [30000/60000] Loss: 2.282132148742676\n",
      "Train Epoch: 7 [31000/60000] Loss: 2.2701351642608643\n",
      "Train Epoch: 7 [32000/60000] Loss: 2.273942470550537\n",
      "Train Epoch: 7 [33000/60000] Loss: 2.2707865238189697\n",
      "Train Epoch: 7 [34000/60000] Loss: 2.263216972351074\n",
      "Train Epoch: 7 [35000/60000] Loss: 2.2707560062408447\n",
      "Train Epoch: 7 [36000/60000] Loss: 2.2517738342285156\n",
      "Train Epoch: 7 [37000/60000] Loss: 2.272280693054199\n",
      "Train Epoch: 7 [38000/60000] Loss: 2.2588889598846436\n",
      "Train Epoch: 7 [39000/60000] Loss: 2.278193712234497\n",
      "Train Epoch: 7 [40000/60000] Loss: 2.2612667083740234\n",
      "Train Epoch: 7 [41000/60000] Loss: 2.258157253265381\n",
      "Train Epoch: 7 [42000/60000] Loss: 2.2717337608337402\n",
      "Train Epoch: 7 [43000/60000] Loss: 2.2685177326202393\n",
      "Train Epoch: 7 [44000/60000] Loss: 2.2545576095581055\n",
      "Train Epoch: 7 [45000/60000] Loss: 2.2690651416778564\n",
      "Train Epoch: 7 [46000/60000] Loss: 2.2735419273376465\n",
      "Train Epoch: 7 [47000/60000] Loss: 2.2852416038513184\n",
      "Train Epoch: 7 [48000/60000] Loss: 2.262049674987793\n",
      "Train Epoch: 7 [49000/60000] Loss: 2.255026340484619\n",
      "Train Epoch: 7 [50000/60000] Loss: 2.260951280593872\n",
      "Train Epoch: 7 [51000/60000] Loss: 2.2648792266845703\n",
      "Train Epoch: 7 [52000/60000] Loss: 2.2609305381774902\n",
      "Train Epoch: 7 [53000/60000] Loss: 2.278764009475708\n",
      "Train Epoch: 7 [54000/60000] Loss: 2.2547218799591064\n",
      "Train Epoch: 7 [55000/60000] Loss: 2.278134822845459\n",
      "Train Epoch: 7 [56000/60000] Loss: 2.2652056217193604\n",
      "Train Epoch: 7 [57000/60000] Loss: 2.2561168670654297\n",
      "Train Epoch: 7 [58000/60000] Loss: 2.2762656211853027\n",
      "Train Epoch: 7 [59000/60000] Loss: 2.264622449874878\n",
      "\n",
      "Test set: Average loss: 0.022534208035469055, Accuracy: 4067/10000 (40.67)\n",
      "\n",
      "Train Epoch: 8 [0/60000] Loss: 2.2595958709716797\n",
      "Train Epoch: 8 [1000/60000] Loss: 2.268662929534912\n",
      "Train Epoch: 8 [2000/60000] Loss: 2.2427399158477783\n",
      "Train Epoch: 8 [3000/60000] Loss: 2.2627274990081787\n",
      "Train Epoch: 8 [4000/60000] Loss: 2.2627973556518555\n",
      "Train Epoch: 8 [5000/60000] Loss: 2.2793233394622803\n",
      "Train Epoch: 8 [6000/60000] Loss: 2.274812698364258\n",
      "Train Epoch: 8 [7000/60000] Loss: 2.2422924041748047\n",
      "Train Epoch: 8 [8000/60000] Loss: 2.2495484352111816\n",
      "Train Epoch: 8 [9000/60000] Loss: 2.2638700008392334\n",
      "Train Epoch: 8 [10000/60000] Loss: 2.2574803829193115\n",
      "Train Epoch: 8 [11000/60000] Loss: 2.2655344009399414\n",
      "Train Epoch: 8 [12000/60000] Loss: 2.285522699356079\n",
      "Train Epoch: 8 [13000/60000] Loss: 2.26151967048645\n",
      "Train Epoch: 8 [14000/60000] Loss: 2.2660603523254395\n",
      "Train Epoch: 8 [15000/60000] Loss: 2.2721314430236816\n",
      "Train Epoch: 8 [16000/60000] Loss: 2.2705891132354736\n",
      "Train Epoch: 8 [17000/60000] Loss: 2.242934226989746\n",
      "Train Epoch: 8 [18000/60000] Loss: 2.250316619873047\n",
      "Train Epoch: 8 [19000/60000] Loss: 2.25933575630188\n",
      "Train Epoch: 8 [20000/60000] Loss: 2.250359296798706\n",
      "Train Epoch: 8 [21000/60000] Loss: 2.243079662322998\n",
      "Train Epoch: 8 [22000/60000] Loss: 2.2542994022369385\n",
      "Train Epoch: 8 [23000/60000] Loss: 2.260460138320923\n",
      "Train Epoch: 8 [24000/60000] Loss: 2.2413864135742188\n",
      "Train Epoch: 8 [25000/60000] Loss: 2.247769832611084\n",
      "Train Epoch: 8 [26000/60000] Loss: 2.261634588241577\n",
      "Train Epoch: 8 [27000/60000] Loss: 2.2647006511688232\n",
      "Train Epoch: 8 [28000/60000] Loss: 2.257998466491699\n",
      "Train Epoch: 8 [29000/60000] Loss: 2.248251438140869\n",
      "Train Epoch: 8 [30000/60000] Loss: 2.241682767868042\n",
      "Train Epoch: 8 [31000/60000] Loss: 2.25915789604187\n",
      "Train Epoch: 8 [32000/60000] Loss: 2.257167339324951\n",
      "Train Epoch: 8 [33000/60000] Loss: 2.2609848976135254\n",
      "Train Epoch: 8 [34000/60000] Loss: 2.2477920055389404\n",
      "Train Epoch: 8 [35000/60000] Loss: 2.270577907562256\n",
      "Train Epoch: 8 [36000/60000] Loss: 2.235112190246582\n",
      "Train Epoch: 8 [37000/60000] Loss: 2.245246171951294\n",
      "Train Epoch: 8 [38000/60000] Loss: 2.2395682334899902\n",
      "Train Epoch: 8 [39000/60000] Loss: 2.2564871311187744\n",
      "Train Epoch: 8 [40000/60000] Loss: 2.2411231994628906\n",
      "Train Epoch: 8 [41000/60000] Loss: 2.235201597213745\n",
      "Train Epoch: 8 [42000/60000] Loss: 2.241229772567749\n",
      "Train Epoch: 8 [43000/60000] Loss: 2.231887102127075\n",
      "Train Epoch: 8 [44000/60000] Loss: 2.2348198890686035\n",
      "Train Epoch: 8 [45000/60000] Loss: 2.2479469776153564\n",
      "Train Epoch: 8 [46000/60000] Loss: 2.250563859939575\n",
      "Train Epoch: 8 [47000/60000] Loss: 2.239494562149048\n",
      "Train Epoch: 8 [48000/60000] Loss: 2.2581658363342285\n",
      "Train Epoch: 8 [49000/60000] Loss: 2.243184804916382\n",
      "Train Epoch: 8 [50000/60000] Loss: 2.2381415367126465\n",
      "Train Epoch: 8 [51000/60000] Loss: 2.2362051010131836\n",
      "Train Epoch: 8 [52000/60000] Loss: 2.2410717010498047\n",
      "Train Epoch: 8 [53000/60000] Loss: 2.239377498626709\n",
      "Train Epoch: 8 [54000/60000] Loss: 2.2555322647094727\n",
      "Train Epoch: 8 [55000/60000] Loss: 2.2564499378204346\n",
      "Train Epoch: 8 [56000/60000] Loss: 2.2393648624420166\n",
      "Train Epoch: 8 [57000/60000] Loss: 2.2334625720977783\n",
      "Train Epoch: 8 [58000/60000] Loss: 2.247209072113037\n",
      "Train Epoch: 8 [59000/60000] Loss: 2.229630708694458\n",
      "\n",
      "Test set: Average loss: 0.022268622779846193, Accuracy: 4800/10000 (48.0)\n",
      "\n",
      "Train Epoch: 9 [0/60000] Loss: 2.23642635345459\n",
      "Train Epoch: 9 [1000/60000] Loss: 2.22023868560791\n",
      "Train Epoch: 9 [2000/60000] Loss: 2.24019718170166\n",
      "Train Epoch: 9 [3000/60000] Loss: 2.2227776050567627\n",
      "Train Epoch: 9 [4000/60000] Loss: 2.248229742050171\n",
      "Train Epoch: 9 [5000/60000] Loss: 2.2340848445892334\n",
      "Train Epoch: 9 [6000/60000] Loss: 2.2337310314178467\n",
      "Train Epoch: 9 [7000/60000] Loss: 2.2381398677825928\n",
      "Train Epoch: 9 [8000/60000] Loss: 2.2419936656951904\n",
      "Train Epoch: 9 [9000/60000] Loss: 2.2246618270874023\n",
      "Train Epoch: 9 [10000/60000] Loss: 2.248767375946045\n",
      "Train Epoch: 9 [11000/60000] Loss: 2.253974437713623\n",
      "Train Epoch: 9 [12000/60000] Loss: 2.2220354080200195\n",
      "Train Epoch: 9 [13000/60000] Loss: 2.212355136871338\n",
      "Train Epoch: 9 [14000/60000] Loss: 2.2422075271606445\n",
      "Train Epoch: 9 [15000/60000] Loss: 2.2263894081115723\n",
      "Train Epoch: 9 [16000/60000] Loss: 2.251803398132324\n",
      "Train Epoch: 9 [17000/60000] Loss: 2.217628002166748\n",
      "Train Epoch: 9 [18000/60000] Loss: 2.2115836143493652\n",
      "Train Epoch: 9 [19000/60000] Loss: 2.2223968505859375\n",
      "Train Epoch: 9 [20000/60000] Loss: 2.249445676803589\n",
      "Train Epoch: 9 [21000/60000] Loss: 2.2255468368530273\n",
      "Train Epoch: 9 [22000/60000] Loss: 2.202758550643921\n",
      "Train Epoch: 9 [23000/60000] Loss: 2.232210636138916\n",
      "Train Epoch: 9 [24000/60000] Loss: 2.2317402362823486\n",
      "Train Epoch: 9 [25000/60000] Loss: 2.229848861694336\n",
      "Train Epoch: 9 [26000/60000] Loss: 2.2189388275146484\n",
      "Train Epoch: 9 [27000/60000] Loss: 2.2512047290802\n",
      "Train Epoch: 9 [28000/60000] Loss: 2.216540575027466\n",
      "Train Epoch: 9 [29000/60000] Loss: 2.2044620513916016\n",
      "Train Epoch: 9 [30000/60000] Loss: 2.2004616260528564\n",
      "Train Epoch: 9 [31000/60000] Loss: 2.2552738189697266\n",
      "Train Epoch: 9 [32000/60000] Loss: 2.2346043586730957\n",
      "Train Epoch: 9 [33000/60000] Loss: 2.231753349304199\n",
      "Train Epoch: 9 [34000/60000] Loss: 2.1955549716949463\n",
      "Train Epoch: 9 [35000/60000] Loss: 2.2266952991485596\n",
      "Train Epoch: 9 [36000/60000] Loss: 2.2177083492279053\n",
      "Train Epoch: 9 [37000/60000] Loss: 2.231674909591675\n",
      "Train Epoch: 9 [38000/60000] Loss: 2.230236291885376\n",
      "Train Epoch: 9 [39000/60000] Loss: 2.2081148624420166\n",
      "Train Epoch: 9 [40000/60000] Loss: 2.2102231979370117\n",
      "Train Epoch: 9 [41000/60000] Loss: 2.2087149620056152\n",
      "Train Epoch: 9 [42000/60000] Loss: 2.1998956203460693\n",
      "Train Epoch: 9 [43000/60000] Loss: 2.222545862197876\n",
      "Train Epoch: 9 [44000/60000] Loss: 2.2200112342834473\n",
      "Train Epoch: 9 [45000/60000] Loss: 2.197584629058838\n",
      "Train Epoch: 9 [46000/60000] Loss: 2.211214303970337\n",
      "Train Epoch: 9 [47000/60000] Loss: 2.200472831726074\n",
      "Train Epoch: 9 [48000/60000] Loss: 2.206547737121582\n",
      "Train Epoch: 9 [49000/60000] Loss: 2.2001137733459473\n",
      "Train Epoch: 9 [50000/60000] Loss: 2.183600425720215\n",
      "Train Epoch: 9 [51000/60000] Loss: 2.2378385066986084\n",
      "Train Epoch: 9 [52000/60000] Loss: 2.2039237022399902\n",
      "Train Epoch: 9 [53000/60000] Loss: 2.190561532974243\n",
      "Train Epoch: 9 [54000/60000] Loss: 2.1780686378479004\n",
      "Train Epoch: 9 [55000/60000] Loss: 2.1847870349884033\n",
      "Train Epoch: 9 [56000/60000] Loss: 2.224940061569214\n",
      "Train Epoch: 9 [57000/60000] Loss: 2.2082877159118652\n",
      "Train Epoch: 9 [58000/60000] Loss: 2.1912572383880615\n",
      "Train Epoch: 9 [59000/60000] Loss: 2.219433069229126\n",
      "\n",
      "Test set: Average loss: 0.021798689699172973, Accuracy: 5760/10000 (57.6)\n",
      "\n",
      "Train Epoch: 10 [0/60000] Loss: 2.2126190662384033\n",
      "Train Epoch: 10 [1000/60000] Loss: 2.1959495544433594\n",
      "Train Epoch: 10 [2000/60000] Loss: 2.1887688636779785\n",
      "Train Epoch: 10 [3000/60000] Loss: 2.1984336376190186\n",
      "Train Epoch: 10 [4000/60000] Loss: 2.184189558029175\n",
      "Train Epoch: 10 [5000/60000] Loss: 2.1867287158966064\n",
      "Train Epoch: 10 [6000/60000] Loss: 2.170088768005371\n",
      "Train Epoch: 10 [7000/60000] Loss: 2.2113595008850098\n",
      "Train Epoch: 10 [8000/60000] Loss: 2.187030553817749\n",
      "Train Epoch: 10 [9000/60000] Loss: 2.1599385738372803\n",
      "Train Epoch: 10 [10000/60000] Loss: 2.1730470657348633\n",
      "Train Epoch: 10 [11000/60000] Loss: 2.190323829650879\n",
      "Train Epoch: 10 [12000/60000] Loss: 2.1930627822875977\n",
      "Train Epoch: 10 [13000/60000] Loss: 2.198298454284668\n",
      "Train Epoch: 10 [14000/60000] Loss: 2.1943001747131348\n",
      "Train Epoch: 10 [15000/60000] Loss: 2.15910005569458\n",
      "Train Epoch: 10 [16000/60000] Loss: 2.148642063140869\n",
      "Train Epoch: 10 [17000/60000] Loss: 2.195725917816162\n",
      "Train Epoch: 10 [18000/60000] Loss: 2.1621227264404297\n",
      "Train Epoch: 10 [19000/60000] Loss: 2.1696555614471436\n",
      "Train Epoch: 10 [20000/60000] Loss: 2.1941745281219482\n",
      "Train Epoch: 10 [21000/60000] Loss: 2.2050676345825195\n",
      "Train Epoch: 10 [22000/60000] Loss: 2.2101497650146484\n",
      "Train Epoch: 10 [23000/60000] Loss: 2.1880571842193604\n",
      "Train Epoch: 10 [24000/60000] Loss: 2.194929361343384\n",
      "Train Epoch: 10 [25000/60000] Loss: 2.188136339187622\n",
      "Train Epoch: 10 [26000/60000] Loss: 2.1879465579986572\n",
      "Train Epoch: 10 [27000/60000] Loss: 2.171563148498535\n",
      "Train Epoch: 10 [28000/60000] Loss: 2.197756290435791\n",
      "Train Epoch: 10 [29000/60000] Loss: 2.1683788299560547\n",
      "Train Epoch: 10 [30000/60000] Loss: 2.17997670173645\n",
      "Train Epoch: 10 [31000/60000] Loss: 2.157444715499878\n",
      "Train Epoch: 10 [32000/60000] Loss: 2.2011959552764893\n",
      "Train Epoch: 10 [33000/60000] Loss: 2.1878044605255127\n",
      "Train Epoch: 10 [34000/60000] Loss: 2.1862456798553467\n",
      "Train Epoch: 10 [35000/60000] Loss: 2.147817611694336\n",
      "Train Epoch: 10 [36000/60000] Loss: 2.189570903778076\n",
      "Train Epoch: 10 [37000/60000] Loss: 2.182882785797119\n",
      "Train Epoch: 10 [38000/60000] Loss: 2.158693790435791\n",
      "Train Epoch: 10 [39000/60000] Loss: 2.1245648860931396\n",
      "Train Epoch: 10 [40000/60000] Loss: 2.2065765857696533\n",
      "Train Epoch: 10 [41000/60000] Loss: 2.2010319232940674\n",
      "Train Epoch: 10 [42000/60000] Loss: 2.164278745651245\n",
      "Train Epoch: 10 [43000/60000] Loss: 2.152812957763672\n",
      "Train Epoch: 10 [44000/60000] Loss: 2.1478285789489746\n",
      "Train Epoch: 10 [45000/60000] Loss: 2.1876847743988037\n",
      "Train Epoch: 10 [46000/60000] Loss: 2.1765334606170654\n",
      "Train Epoch: 10 [47000/60000] Loss: 2.1438095569610596\n",
      "Train Epoch: 10 [48000/60000] Loss: 2.1750364303588867\n",
      "Train Epoch: 10 [49000/60000] Loss: 2.156661033630371\n",
      "Train Epoch: 10 [50000/60000] Loss: 2.134834051132202\n",
      "Train Epoch: 10 [51000/60000] Loss: 2.170609951019287\n",
      "Train Epoch: 10 [52000/60000] Loss: 2.1489837169647217\n",
      "Train Epoch: 10 [53000/60000] Loss: 2.140876293182373\n",
      "Train Epoch: 10 [54000/60000] Loss: 2.173508405685425\n",
      "Train Epoch: 10 [55000/60000] Loss: 2.1591129302978516\n",
      "Train Epoch: 10 [56000/60000] Loss: 2.1558306217193604\n",
      "Train Epoch: 10 [57000/60000] Loss: 2.1497573852539062\n",
      "Train Epoch: 10 [58000/60000] Loss: 2.133669853210449\n",
      "Train Epoch: 10 [59000/60000] Loss: 2.1322970390319824\n",
      "\n",
      "Test set: Average loss: 0.02092938439846039, Accuracy: 6343/10000 (63.43)\n",
      "\n",
      "Train Epoch: 11 [0/60000] Loss: 2.1502466201782227\n",
      "Train Epoch: 11 [1000/60000] Loss: 2.1460225582122803\n",
      "Train Epoch: 11 [2000/60000] Loss: 2.1902668476104736\n",
      "Train Epoch: 11 [3000/60000] Loss: 2.161092758178711\n",
      "Train Epoch: 11 [4000/60000] Loss: 2.122924566268921\n",
      "Train Epoch: 11 [5000/60000] Loss: 2.1367735862731934\n",
      "Train Epoch: 11 [6000/60000] Loss: 2.0991909503936768\n",
      "Train Epoch: 11 [7000/60000] Loss: 2.1010706424713135\n",
      "Train Epoch: 11 [8000/60000] Loss: 2.068532943725586\n",
      "Train Epoch: 11 [9000/60000] Loss: 2.104989528656006\n",
      "Train Epoch: 11 [10000/60000] Loss: 2.1188509464263916\n",
      "Train Epoch: 11 [11000/60000] Loss: 2.077563762664795\n",
      "Train Epoch: 11 [12000/60000] Loss: 2.155977249145508\n",
      "Train Epoch: 11 [13000/60000] Loss: 2.0957000255584717\n",
      "Train Epoch: 11 [14000/60000] Loss: 2.1201443672180176\n",
      "Train Epoch: 11 [15000/60000] Loss: 2.0622782707214355\n",
      "Train Epoch: 11 [16000/60000] Loss: 2.145453453063965\n",
      "Train Epoch: 11 [17000/60000] Loss: 2.1043622493743896\n",
      "Train Epoch: 11 [18000/60000] Loss: 2.126850128173828\n",
      "Train Epoch: 11 [19000/60000] Loss: 2.1474404335021973\n",
      "Train Epoch: 11 [20000/60000] Loss: 2.0917465686798096\n",
      "Train Epoch: 11 [21000/60000] Loss: 2.114449977874756\n",
      "Train Epoch: 11 [22000/60000] Loss: 2.1295981407165527\n",
      "Train Epoch: 11 [23000/60000] Loss: 2.137197732925415\n",
      "Train Epoch: 11 [24000/60000] Loss: 2.11875581741333\n",
      "Train Epoch: 11 [25000/60000] Loss: 2.09816837310791\n",
      "Train Epoch: 11 [26000/60000] Loss: 2.1078052520751953\n",
      "Train Epoch: 11 [27000/60000] Loss: 2.0762240886688232\n",
      "Train Epoch: 11 [28000/60000] Loss: 2.0760114192962646\n",
      "Train Epoch: 11 [29000/60000] Loss: 2.1197350025177\n",
      "Train Epoch: 11 [30000/60000] Loss: 2.107560873031616\n",
      "Train Epoch: 11 [31000/60000] Loss: 2.0925817489624023\n",
      "Train Epoch: 11 [32000/60000] Loss: 2.084066390991211\n",
      "Train Epoch: 11 [33000/60000] Loss: 2.0723347663879395\n",
      "Train Epoch: 11 [34000/60000] Loss: 2.0755701065063477\n",
      "Train Epoch: 11 [35000/60000] Loss: 2.096731185913086\n",
      "Train Epoch: 11 [36000/60000] Loss: 1.9889625310897827\n",
      "Train Epoch: 11 [37000/60000] Loss: 2.1027767658233643\n",
      "Train Epoch: 11 [38000/60000] Loss: 2.0940308570861816\n",
      "Train Epoch: 11 [39000/60000] Loss: 2.0140252113342285\n",
      "Train Epoch: 11 [40000/60000] Loss: 2.115082263946533\n",
      "Train Epoch: 11 [41000/60000] Loss: 2.0425021648406982\n",
      "Train Epoch: 11 [42000/60000] Loss: 2.055384874343872\n",
      "Train Epoch: 11 [43000/60000] Loss: 2.015564441680908\n",
      "Train Epoch: 11 [44000/60000] Loss: 2.0631070137023926\n",
      "Train Epoch: 11 [45000/60000] Loss: 2.002277374267578\n",
      "Train Epoch: 11 [46000/60000] Loss: 2.0749082565307617\n",
      "Train Epoch: 11 [47000/60000] Loss: 2.1097707748413086\n",
      "Train Epoch: 11 [48000/60000] Loss: 2.006516933441162\n",
      "Train Epoch: 11 [49000/60000] Loss: 1.9633690118789673\n",
      "Train Epoch: 11 [50000/60000] Loss: 2.055368185043335\n",
      "Train Epoch: 11 [51000/60000] Loss: 2.0174334049224854\n",
      "Train Epoch: 11 [52000/60000] Loss: 2.023935317993164\n",
      "Train Epoch: 11 [53000/60000] Loss: 2.061450719833374\n",
      "Train Epoch: 11 [54000/60000] Loss: 2.0060102939605713\n",
      "Train Epoch: 11 [55000/60000] Loss: 2.028965950012207\n",
      "Train Epoch: 11 [56000/60000] Loss: 2.0088415145874023\n",
      "Train Epoch: 11 [57000/60000] Loss: 1.985063910484314\n",
      "Train Epoch: 11 [58000/60000] Loss: 1.9146227836608887\n",
      "Train Epoch: 11 [59000/60000] Loss: 1.97210693359375\n",
      "\n",
      "Test set: Average loss: 0.019212415027618408, Accuracy: 6919/10000 (69.19)\n",
      "\n",
      "Train Epoch: 12 [0/60000] Loss: 2.1056442260742188\n",
      "Train Epoch: 12 [1000/60000] Loss: 1.9193694591522217\n",
      "Train Epoch: 12 [2000/60000] Loss: 2.0339701175689697\n",
      "Train Epoch: 12 [3000/60000] Loss: 2.0389726161956787\n",
      "Train Epoch: 12 [4000/60000] Loss: 1.9646964073181152\n",
      "Train Epoch: 12 [5000/60000] Loss: 2.0533907413482666\n",
      "Train Epoch: 12 [6000/60000] Loss: 2.0181572437286377\n",
      "Train Epoch: 12 [7000/60000] Loss: 1.994390606880188\n",
      "Train Epoch: 12 [8000/60000] Loss: 1.9527359008789062\n",
      "Train Epoch: 12 [9000/60000] Loss: 2.0063748359680176\n",
      "Train Epoch: 12 [10000/60000] Loss: 2.029921531677246\n",
      "Train Epoch: 12 [11000/60000] Loss: 1.9544583559036255\n",
      "Train Epoch: 12 [12000/60000] Loss: 1.9688410758972168\n",
      "Train Epoch: 12 [13000/60000] Loss: 1.9667474031448364\n",
      "Train Epoch: 12 [14000/60000] Loss: 1.9403598308563232\n",
      "Train Epoch: 12 [15000/60000] Loss: 1.8116612434387207\n",
      "Train Epoch: 12 [16000/60000] Loss: 1.9080843925476074\n",
      "Train Epoch: 12 [17000/60000] Loss: 1.9060935974121094\n",
      "Train Epoch: 12 [18000/60000] Loss: 1.9761825799942017\n",
      "Train Epoch: 12 [19000/60000] Loss: 1.9993191957473755\n",
      "Train Epoch: 12 [20000/60000] Loss: 1.9446556568145752\n",
      "Train Epoch: 12 [21000/60000] Loss: 1.9792981147766113\n",
      "Train Epoch: 12 [22000/60000] Loss: 1.8448272943496704\n",
      "Train Epoch: 12 [23000/60000] Loss: 1.9678159952163696\n",
      "Train Epoch: 12 [24000/60000] Loss: 1.868808388710022\n",
      "Train Epoch: 12 [25000/60000] Loss: 1.9526442289352417\n",
      "Train Epoch: 12 [26000/60000] Loss: 1.9353159666061401\n",
      "Train Epoch: 12 [27000/60000] Loss: 1.9495042562484741\n",
      "Train Epoch: 12 [28000/60000] Loss: 1.928030252456665\n",
      "Train Epoch: 12 [29000/60000] Loss: 1.8609728813171387\n",
      "Train Epoch: 12 [30000/60000] Loss: 1.9389256238937378\n",
      "Train Epoch: 12 [31000/60000] Loss: 1.9635096788406372\n",
      "Train Epoch: 12 [32000/60000] Loss: 1.972719669342041\n",
      "Train Epoch: 12 [33000/60000] Loss: 1.893221139907837\n",
      "Train Epoch: 12 [34000/60000] Loss: 1.9073299169540405\n",
      "Train Epoch: 12 [35000/60000] Loss: 1.8437854051589966\n",
      "Train Epoch: 12 [36000/60000] Loss: 1.922452449798584\n",
      "Train Epoch: 12 [37000/60000] Loss: 1.8270821571350098\n",
      "Train Epoch: 12 [38000/60000] Loss: 1.8566715717315674\n",
      "Train Epoch: 12 [39000/60000] Loss: 1.885996699333191\n",
      "Train Epoch: 12 [40000/60000] Loss: 1.8971813917160034\n",
      "Train Epoch: 12 [41000/60000] Loss: 2.0114591121673584\n",
      "Train Epoch: 12 [42000/60000] Loss: 1.8400697708129883\n",
      "Train Epoch: 12 [43000/60000] Loss: 1.873172402381897\n",
      "Train Epoch: 12 [44000/60000] Loss: 1.9181952476501465\n",
      "Train Epoch: 12 [45000/60000] Loss: 1.8158445358276367\n",
      "Train Epoch: 12 [46000/60000] Loss: 1.9197196960449219\n",
      "Train Epoch: 12 [47000/60000] Loss: 1.867812991142273\n",
      "Train Epoch: 12 [48000/60000] Loss: 1.8808276653289795\n",
      "Train Epoch: 12 [49000/60000] Loss: 1.8403387069702148\n",
      "Train Epoch: 12 [50000/60000] Loss: 1.7221602201461792\n",
      "Train Epoch: 12 [51000/60000] Loss: 1.8091448545455933\n",
      "Train Epoch: 12 [52000/60000] Loss: 1.8617384433746338\n",
      "Train Epoch: 12 [53000/60000] Loss: 1.848386526107788\n",
      "Train Epoch: 12 [54000/60000] Loss: 1.9238953590393066\n",
      "Train Epoch: 12 [55000/60000] Loss: 1.8202701807022095\n",
      "Train Epoch: 12 [56000/60000] Loss: 1.848934292793274\n",
      "Train Epoch: 12 [57000/60000] Loss: 1.8020182847976685\n",
      "Train Epoch: 12 [58000/60000] Loss: 1.812781810760498\n",
      "Train Epoch: 12 [59000/60000] Loss: 1.7690223455429077\n",
      "\n",
      "Test set: Average loss: 0.016278814780712126, Accuracy: 7497/10000 (74.97)\n",
      "\n",
      "Train Epoch: 13 [0/60000] Loss: 1.8001036643981934\n",
      "Train Epoch: 13 [1000/60000] Loss: 1.8296725749969482\n",
      "Train Epoch: 13 [2000/60000] Loss: 1.6958911418914795\n",
      "Train Epoch: 13 [3000/60000] Loss: 1.766850233078003\n",
      "Train Epoch: 13 [4000/60000] Loss: 1.7581498622894287\n",
      "Train Epoch: 13 [5000/60000] Loss: 1.6884253025054932\n",
      "Train Epoch: 13 [6000/60000] Loss: 1.8303982019424438\n",
      "Train Epoch: 13 [7000/60000] Loss: 1.6725459098815918\n",
      "Train Epoch: 13 [8000/60000] Loss: 1.8334627151489258\n",
      "Train Epoch: 13 [9000/60000] Loss: 1.6645421981811523\n",
      "Train Epoch: 13 [10000/60000] Loss: 1.7604849338531494\n",
      "Train Epoch: 13 [11000/60000] Loss: 1.8640971183776855\n",
      "Train Epoch: 13 [12000/60000] Loss: 1.7382545471191406\n",
      "Train Epoch: 13 [13000/60000] Loss: 1.6786788702011108\n",
      "Train Epoch: 13 [14000/60000] Loss: 1.6871752738952637\n",
      "Train Epoch: 13 [15000/60000] Loss: 1.8003979921340942\n",
      "Train Epoch: 13 [16000/60000] Loss: 1.707699179649353\n",
      "Train Epoch: 13 [17000/60000] Loss: 1.6063599586486816\n",
      "Train Epoch: 13 [18000/60000] Loss: 1.6491442918777466\n",
      "Train Epoch: 13 [19000/60000] Loss: 1.7235100269317627\n",
      "Train Epoch: 13 [20000/60000] Loss: 1.6476627588272095\n",
      "Train Epoch: 13 [21000/60000] Loss: 1.7526887655258179\n",
      "Train Epoch: 13 [22000/60000] Loss: 1.6563349962234497\n",
      "Train Epoch: 13 [23000/60000] Loss: 1.8061293363571167\n",
      "Train Epoch: 13 [24000/60000] Loss: 1.732591152191162\n",
      "Train Epoch: 13 [25000/60000] Loss: 1.6707212924957275\n",
      "Train Epoch: 13 [26000/60000] Loss: 1.6285300254821777\n",
      "Train Epoch: 13 [27000/60000] Loss: 1.764710545539856\n",
      "Train Epoch: 13 [28000/60000] Loss: 1.6294937133789062\n",
      "Train Epoch: 13 [29000/60000] Loss: 1.7963756322860718\n",
      "Train Epoch: 13 [30000/60000] Loss: 1.6719589233398438\n",
      "Train Epoch: 13 [31000/60000] Loss: 1.6336889266967773\n",
      "Train Epoch: 13 [32000/60000] Loss: 1.6146306991577148\n",
      "Train Epoch: 13 [33000/60000] Loss: 1.6005150079727173\n",
      "Train Epoch: 13 [34000/60000] Loss: 1.6318920850753784\n",
      "Train Epoch: 13 [35000/60000] Loss: 1.6041557788848877\n",
      "Train Epoch: 13 [36000/60000] Loss: 1.6719778776168823\n",
      "Train Epoch: 13 [37000/60000] Loss: 1.5815118551254272\n",
      "Train Epoch: 13 [38000/60000] Loss: 1.545291543006897\n",
      "Train Epoch: 13 [39000/60000] Loss: 1.5630055665969849\n",
      "Train Epoch: 13 [40000/60000] Loss: 1.6259384155273438\n",
      "Train Epoch: 13 [41000/60000] Loss: 1.5979588031768799\n",
      "Train Epoch: 13 [42000/60000] Loss: 1.5315133333206177\n",
      "Train Epoch: 13 [43000/60000] Loss: 1.5600379705429077\n",
      "Train Epoch: 13 [44000/60000] Loss: 1.645166277885437\n",
      "Train Epoch: 13 [45000/60000] Loss: 1.6105002164840698\n",
      "Train Epoch: 13 [46000/60000] Loss: 1.5856224298477173\n",
      "Train Epoch: 13 [47000/60000] Loss: 1.603018045425415\n",
      "Train Epoch: 13 [48000/60000] Loss: 1.6674354076385498\n",
      "Train Epoch: 13 [49000/60000] Loss: 1.6451650857925415\n",
      "Train Epoch: 13 [50000/60000] Loss: 1.7281588315963745\n",
      "Train Epoch: 13 [51000/60000] Loss: 1.6235219240188599\n",
      "Train Epoch: 13 [52000/60000] Loss: 1.57915461063385\n",
      "Train Epoch: 13 [53000/60000] Loss: 1.5815962553024292\n",
      "Train Epoch: 13 [54000/60000] Loss: 1.5885014533996582\n",
      "Train Epoch: 13 [55000/60000] Loss: 1.5125906467437744\n",
      "Train Epoch: 13 [56000/60000] Loss: 1.5868000984191895\n",
      "Train Epoch: 13 [57000/60000] Loss: 1.4636012315750122\n",
      "Train Epoch: 13 [58000/60000] Loss: 1.5770277976989746\n",
      "Train Epoch: 13 [59000/60000] Loss: 1.4921694993972778\n",
      "\n",
      "Test set: Average loss: 0.01275337462425232, Accuracy: 7827/10000 (78.27)\n",
      "\n",
      "Train Epoch: 14 [0/60000] Loss: 1.6357219219207764\n",
      "Train Epoch: 14 [1000/60000] Loss: 1.4190757274627686\n",
      "Train Epoch: 14 [2000/60000] Loss: 1.5899206399917603\n",
      "Train Epoch: 14 [3000/60000] Loss: 1.575832486152649\n",
      "Train Epoch: 14 [4000/60000] Loss: 1.4952527284622192\n",
      "Train Epoch: 14 [5000/60000] Loss: 1.3351969718933105\n",
      "Train Epoch: 14 [6000/60000] Loss: 1.647762417793274\n",
      "Train Epoch: 14 [7000/60000] Loss: 1.5852824449539185\n",
      "Train Epoch: 14 [8000/60000] Loss: 1.630630612373352\n",
      "Train Epoch: 14 [9000/60000] Loss: 1.5165183544158936\n",
      "Train Epoch: 14 [10000/60000] Loss: 1.450576901435852\n",
      "Train Epoch: 14 [11000/60000] Loss: 1.5714884996414185\n",
      "Train Epoch: 14 [12000/60000] Loss: 1.5896389484405518\n",
      "Train Epoch: 14 [13000/60000] Loss: 1.5123835802078247\n",
      "Train Epoch: 14 [14000/60000] Loss: 1.5045573711395264\n",
      "Train Epoch: 14 [15000/60000] Loss: 1.4923248291015625\n",
      "Train Epoch: 14 [16000/60000] Loss: 1.4541891813278198\n",
      "Train Epoch: 14 [17000/60000] Loss: 1.5484657287597656\n",
      "Train Epoch: 14 [18000/60000] Loss: 1.5752406120300293\n",
      "Train Epoch: 14 [19000/60000] Loss: 1.490169644355774\n",
      "Train Epoch: 14 [20000/60000] Loss: 1.5246169567108154\n",
      "Train Epoch: 14 [21000/60000] Loss: 1.4282220602035522\n",
      "Train Epoch: 14 [22000/60000] Loss: 1.5308938026428223\n",
      "Train Epoch: 14 [23000/60000] Loss: 1.4337390661239624\n",
      "Train Epoch: 14 [24000/60000] Loss: 1.378818154335022\n",
      "Train Epoch: 14 [25000/60000] Loss: 1.5161182880401611\n",
      "Train Epoch: 14 [26000/60000] Loss: 1.5471786260604858\n",
      "Train Epoch: 14 [27000/60000] Loss: 1.541244387626648\n",
      "Train Epoch: 14 [28000/60000] Loss: 1.3967586755752563\n",
      "Train Epoch: 14 [29000/60000] Loss: 1.457699179649353\n",
      "Train Epoch: 14 [30000/60000] Loss: 1.363165020942688\n",
      "Train Epoch: 14 [31000/60000] Loss: 1.4465006589889526\n",
      "Train Epoch: 14 [32000/60000] Loss: 1.2825088500976562\n",
      "Train Epoch: 14 [33000/60000] Loss: 1.3473902940750122\n",
      "Train Epoch: 14 [34000/60000] Loss: 1.3984431028366089\n",
      "Train Epoch: 14 [35000/60000] Loss: 1.6637187004089355\n",
      "Train Epoch: 14 [36000/60000] Loss: 1.414678692817688\n",
      "Train Epoch: 14 [37000/60000] Loss: 1.369520664215088\n",
      "Train Epoch: 14 [38000/60000] Loss: 1.502390742301941\n",
      "Train Epoch: 14 [39000/60000] Loss: 1.370559573173523\n",
      "Train Epoch: 14 [40000/60000] Loss: 1.2920111417770386\n",
      "Train Epoch: 14 [41000/60000] Loss: 1.3579119443893433\n",
      "Train Epoch: 14 [42000/60000] Loss: 1.508919358253479\n",
      "Train Epoch: 14 [43000/60000] Loss: 1.3170535564422607\n",
      "Train Epoch: 14 [44000/60000] Loss: 1.4421372413635254\n",
      "Train Epoch: 14 [45000/60000] Loss: 1.4565602540969849\n",
      "Train Epoch: 14 [46000/60000] Loss: 1.4745815992355347\n",
      "Train Epoch: 14 [47000/60000] Loss: 1.3466482162475586\n",
      "Train Epoch: 14 [48000/60000] Loss: 1.3453975915908813\n",
      "Train Epoch: 14 [49000/60000] Loss: 1.3768484592437744\n",
      "Train Epoch: 14 [50000/60000] Loss: 1.3684316873550415\n",
      "Train Epoch: 14 [51000/60000] Loss: 1.3596816062927246\n",
      "Train Epoch: 14 [52000/60000] Loss: 1.2712950706481934\n",
      "Train Epoch: 14 [53000/60000] Loss: 1.327804684638977\n",
      "Train Epoch: 14 [54000/60000] Loss: 1.2384791374206543\n",
      "Train Epoch: 14 [55000/60000] Loss: 1.3258591890335083\n",
      "Train Epoch: 14 [56000/60000] Loss: 1.4666705131530762\n",
      "Train Epoch: 14 [57000/60000] Loss: 1.3919885158538818\n",
      "Train Epoch: 14 [58000/60000] Loss: 1.3764238357543945\n",
      "Train Epoch: 14 [59000/60000] Loss: 1.3833813667297363\n",
      "\n",
      "Test set: Average loss: 0.00997604667544365, Accuracy: 8070/10000 (80.7)\n",
      "\n",
      "Train Epoch: 15 [0/60000] Loss: 1.240086317062378\n",
      "Train Epoch: 15 [1000/60000] Loss: 1.1887927055358887\n",
      "Train Epoch: 15 [2000/60000] Loss: 1.2811435461044312\n",
      "Train Epoch: 15 [3000/60000] Loss: 1.3340400457382202\n",
      "Train Epoch: 15 [4000/60000] Loss: 1.3443447351455688\n",
      "Train Epoch: 15 [5000/60000] Loss: 1.2308238744735718\n",
      "Train Epoch: 15 [6000/60000] Loss: 1.366400957107544\n",
      "Train Epoch: 15 [7000/60000] Loss: 1.3659802675247192\n",
      "Train Epoch: 15 [8000/60000] Loss: 1.2933844327926636\n",
      "Train Epoch: 15 [9000/60000] Loss: 1.2942696809768677\n",
      "Train Epoch: 15 [10000/60000] Loss: 1.3761428594589233\n",
      "Train Epoch: 15 [11000/60000] Loss: 1.4231773614883423\n",
      "Train Epoch: 15 [12000/60000] Loss: 1.2076294422149658\n",
      "Train Epoch: 15 [13000/60000] Loss: 1.3501826524734497\n",
      "Train Epoch: 15 [14000/60000] Loss: 1.201767921447754\n",
      "Train Epoch: 15 [15000/60000] Loss: 1.3140584230422974\n",
      "Train Epoch: 15 [16000/60000] Loss: 1.2720128297805786\n",
      "Train Epoch: 15 [17000/60000] Loss: 1.2844276428222656\n",
      "Train Epoch: 15 [18000/60000] Loss: 1.2386804819107056\n",
      "Train Epoch: 15 [19000/60000] Loss: 1.111772894859314\n",
      "Train Epoch: 15 [20000/60000] Loss: 1.3559015989303589\n",
      "Train Epoch: 15 [21000/60000] Loss: 1.3606784343719482\n",
      "Train Epoch: 15 [22000/60000] Loss: 1.0706273317337036\n",
      "Train Epoch: 15 [23000/60000] Loss: 1.2138310670852661\n",
      "Train Epoch: 15 [24000/60000] Loss: 1.1609562635421753\n",
      "Train Epoch: 15 [25000/60000] Loss: 1.2857569456100464\n",
      "Train Epoch: 15 [26000/60000] Loss: 1.2029995918273926\n",
      "Train Epoch: 15 [27000/60000] Loss: 1.2550783157348633\n",
      "Train Epoch: 15 [28000/60000] Loss: 1.2548613548278809\n",
      "Train Epoch: 15 [29000/60000] Loss: 1.1112772226333618\n",
      "Train Epoch: 15 [30000/60000] Loss: 1.3180084228515625\n",
      "Train Epoch: 15 [31000/60000] Loss: 1.199263572692871\n",
      "Train Epoch: 15 [32000/60000] Loss: 1.1196266412734985\n",
      "Train Epoch: 15 [33000/60000] Loss: 1.4007577896118164\n",
      "Train Epoch: 15 [34000/60000] Loss: 1.2413333654403687\n",
      "Train Epoch: 15 [35000/60000] Loss: 1.3244060277938843\n",
      "Train Epoch: 15 [36000/60000] Loss: 1.351710557937622\n",
      "Train Epoch: 15 [37000/60000] Loss: 1.183512806892395\n",
      "Train Epoch: 15 [38000/60000] Loss: 1.2805447578430176\n",
      "Train Epoch: 15 [39000/60000] Loss: 1.1068862676620483\n",
      "Train Epoch: 15 [40000/60000] Loss: 1.2398262023925781\n",
      "Train Epoch: 15 [41000/60000] Loss: 1.1099194288253784\n",
      "Train Epoch: 15 [42000/60000] Loss: 1.2143808603286743\n",
      "Train Epoch: 15 [43000/60000] Loss: 1.3257851600646973\n",
      "Train Epoch: 15 [44000/60000] Loss: 1.2222141027450562\n",
      "Train Epoch: 15 [45000/60000] Loss: 1.1637849807739258\n",
      "Train Epoch: 15 [46000/60000] Loss: 1.1428800821304321\n",
      "Train Epoch: 15 [47000/60000] Loss: 1.091127872467041\n",
      "Train Epoch: 15 [48000/60000] Loss: 1.2832651138305664\n",
      "Train Epoch: 15 [49000/60000] Loss: 1.358687400817871\n",
      "Train Epoch: 15 [50000/60000] Loss: 1.2620097398757935\n",
      "Train Epoch: 15 [51000/60000] Loss: 1.2072871923446655\n",
      "Train Epoch: 15 [52000/60000] Loss: 1.3721351623535156\n",
      "Train Epoch: 15 [53000/60000] Loss: 1.2872145175933838\n",
      "Train Epoch: 15 [54000/60000] Loss: 1.169999361038208\n",
      "Train Epoch: 15 [55000/60000] Loss: 1.167665719985962\n",
      "Train Epoch: 15 [56000/60000] Loss: 1.162968635559082\n",
      "Train Epoch: 15 [57000/60000] Loss: 1.121695637702942\n",
      "Train Epoch: 15 [58000/60000] Loss: 1.3095849752426147\n",
      "Train Epoch: 15 [59000/60000] Loss: 1.0559358596801758\n",
      "\n",
      "Test set: Average loss: 0.008083274084329605, Accuracy: 8294/10000 (82.94)\n",
      "\n",
      "Train Epoch: 16 [0/60000] Loss: 1.2048406600952148\n",
      "Train Epoch: 16 [1000/60000] Loss: 1.1635907888412476\n",
      "Train Epoch: 16 [2000/60000] Loss: 1.1464128494262695\n",
      "Train Epoch: 16 [3000/60000] Loss: 1.234311819076538\n",
      "Train Epoch: 16 [4000/60000] Loss: 1.2190359830856323\n",
      "Train Epoch: 16 [5000/60000] Loss: 1.0718810558319092\n",
      "Train Epoch: 16 [6000/60000] Loss: 1.0278403759002686\n",
      "Train Epoch: 16 [7000/60000] Loss: 1.2033514976501465\n",
      "Train Epoch: 16 [8000/60000] Loss: 1.1808276176452637\n",
      "Train Epoch: 16 [9000/60000] Loss: 1.280410647392273\n",
      "Train Epoch: 16 [10000/60000] Loss: 1.3124545812606812\n",
      "Train Epoch: 16 [11000/60000] Loss: 1.171964406967163\n",
      "Train Epoch: 16 [12000/60000] Loss: 1.1914892196655273\n",
      "Train Epoch: 16 [13000/60000] Loss: 1.295005440711975\n",
      "Train Epoch: 16 [14000/60000] Loss: 1.0631015300750732\n",
      "Train Epoch: 16 [15000/60000] Loss: 1.252325415611267\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.9991047382354736\n",
      "Train Epoch: 16 [17000/60000] Loss: 1.2389212846755981\n",
      "Train Epoch: 16 [18000/60000] Loss: 1.1729949712753296\n",
      "Train Epoch: 16 [19000/60000] Loss: 1.1649706363677979\n",
      "Train Epoch: 16 [20000/60000] Loss: 1.235735297203064\n",
      "Train Epoch: 16 [21000/60000] Loss: 0.9885117411613464\n",
      "Train Epoch: 16 [22000/60000] Loss: 1.039004921913147\n",
      "Train Epoch: 16 [23000/60000] Loss: 1.0225094556808472\n",
      "Train Epoch: 16 [24000/60000] Loss: 1.0914719104766846\n",
      "Train Epoch: 16 [25000/60000] Loss: 1.217421293258667\n",
      "Train Epoch: 16 [26000/60000] Loss: 1.174780249595642\n",
      "Train Epoch: 16 [27000/60000] Loss: 1.125367283821106\n",
      "Train Epoch: 16 [28000/60000] Loss: 1.0851249694824219\n",
      "Train Epoch: 16 [29000/60000] Loss: 0.9685753583908081\n",
      "Train Epoch: 16 [30000/60000] Loss: 1.1543556451797485\n",
      "Train Epoch: 16 [31000/60000] Loss: 1.2304683923721313\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.998079240322113\n",
      "Train Epoch: 16 [33000/60000] Loss: 1.1273517608642578\n",
      "Train Epoch: 16 [34000/60000] Loss: 0.9582332372665405\n",
      "Train Epoch: 16 [35000/60000] Loss: 1.2696415185928345\n",
      "Train Epoch: 16 [36000/60000] Loss: 1.1364774703979492\n",
      "Train Epoch: 16 [37000/60000] Loss: 1.1765494346618652\n",
      "Train Epoch: 16 [38000/60000] Loss: 1.0082364082336426\n",
      "Train Epoch: 16 [39000/60000] Loss: 1.193965196609497\n",
      "Train Epoch: 16 [40000/60000] Loss: 1.0724483728408813\n",
      "Train Epoch: 16 [41000/60000] Loss: 0.9388734698295593\n",
      "Train Epoch: 16 [42000/60000] Loss: 1.0704115629196167\n",
      "Train Epoch: 16 [43000/60000] Loss: 1.0842057466506958\n",
      "Train Epoch: 16 [44000/60000] Loss: 1.0010782480239868\n",
      "Train Epoch: 16 [45000/60000] Loss: 1.1204233169555664\n",
      "Train Epoch: 16 [46000/60000] Loss: 1.1150753498077393\n",
      "Train Epoch: 16 [47000/60000] Loss: 1.074119210243225\n",
      "Train Epoch: 16 [48000/60000] Loss: 1.1617472171783447\n",
      "Train Epoch: 16 [49000/60000] Loss: 1.1482146978378296\n",
      "Train Epoch: 16 [50000/60000] Loss: 0.9479517936706543\n",
      "Train Epoch: 16 [51000/60000] Loss: 1.15693998336792\n",
      "Train Epoch: 16 [52000/60000] Loss: 0.9776131510734558\n",
      "Train Epoch: 16 [53000/60000] Loss: 1.2466435432434082\n",
      "Train Epoch: 16 [54000/60000] Loss: 1.0942354202270508\n",
      "Train Epoch: 16 [55000/60000] Loss: 1.0810316801071167\n",
      "Train Epoch: 16 [56000/60000] Loss: 1.1201115846633911\n",
      "Train Epoch: 16 [57000/60000] Loss: 1.093995213508606\n",
      "Train Epoch: 16 [58000/60000] Loss: 0.9896643161773682\n",
      "Train Epoch: 16 [59000/60000] Loss: 1.1000877618789673\n",
      "\n",
      "Test set: Average loss: 0.0068463475406169896, Accuracy: 8446/10000 (84.46)\n",
      "\n",
      "Train Epoch: 17 [0/60000] Loss: 1.13824462890625\n",
      "Train Epoch: 17 [1000/60000] Loss: 1.0692212581634521\n",
      "Train Epoch: 17 [2000/60000] Loss: 0.9690987467765808\n",
      "Train Epoch: 17 [3000/60000] Loss: 1.1894774436950684\n",
      "Train Epoch: 17 [4000/60000] Loss: 1.1129895448684692\n",
      "Train Epoch: 17 [5000/60000] Loss: 0.953117311000824\n",
      "Train Epoch: 17 [6000/60000] Loss: 1.0836161375045776\n",
      "Train Epoch: 17 [7000/60000] Loss: 1.0006136894226074\n",
      "Train Epoch: 17 [8000/60000] Loss: 0.92766273021698\n",
      "Train Epoch: 17 [9000/60000] Loss: 1.0934146642684937\n",
      "Train Epoch: 17 [10000/60000] Loss: 0.9157878756523132\n",
      "Train Epoch: 17 [11000/60000] Loss: 1.0345854759216309\n",
      "Train Epoch: 17 [12000/60000] Loss: 1.0682615041732788\n",
      "Train Epoch: 17 [13000/60000] Loss: 1.096523642539978\n",
      "Train Epoch: 17 [14000/60000] Loss: 0.9840497374534607\n",
      "Train Epoch: 17 [15000/60000] Loss: 1.111191749572754\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.9742530584335327\n",
      "Train Epoch: 17 [17000/60000] Loss: 1.0368411540985107\n",
      "Train Epoch: 17 [18000/60000] Loss: 1.062172532081604\n",
      "Train Epoch: 17 [19000/60000] Loss: 1.0017489194869995\n",
      "Train Epoch: 17 [20000/60000] Loss: 1.0203529596328735\n",
      "Train Epoch: 17 [21000/60000] Loss: 0.917534351348877\n",
      "Train Epoch: 17 [22000/60000] Loss: 1.165259838104248\n",
      "Train Epoch: 17 [23000/60000] Loss: 1.0347005128860474\n",
      "Train Epoch: 17 [24000/60000] Loss: 1.100712537765503\n",
      "Train Epoch: 17 [25000/60000] Loss: 1.0080739259719849\n",
      "Train Epoch: 17 [26000/60000] Loss: 1.1586017608642578\n",
      "Train Epoch: 17 [27000/60000] Loss: 1.2220103740692139\n",
      "Train Epoch: 17 [28000/60000] Loss: 1.0858668088912964\n",
      "Train Epoch: 17 [29000/60000] Loss: 1.0035712718963623\n",
      "Train Epoch: 17 [30000/60000] Loss: 1.030657172203064\n",
      "Train Epoch: 17 [31000/60000] Loss: 1.1273367404937744\n",
      "Train Epoch: 17 [32000/60000] Loss: 1.032188892364502\n",
      "Train Epoch: 17 [33000/60000] Loss: 1.1013988256454468\n",
      "Train Epoch: 17 [34000/60000] Loss: 0.9841839671134949\n",
      "Train Epoch: 17 [35000/60000] Loss: 1.04005765914917\n",
      "Train Epoch: 17 [36000/60000] Loss: 1.059751272201538\n",
      "Train Epoch: 17 [37000/60000] Loss: 1.0742892026901245\n",
      "Train Epoch: 17 [38000/60000] Loss: 1.3719286918640137\n",
      "Train Epoch: 17 [39000/60000] Loss: 0.9287841320037842\n",
      "Train Epoch: 17 [40000/60000] Loss: 0.9756864309310913\n",
      "Train Epoch: 17 [41000/60000] Loss: 1.039470911026001\n",
      "Train Epoch: 17 [42000/60000] Loss: 0.933866560459137\n",
      "Train Epoch: 17 [43000/60000] Loss: 1.1262173652648926\n",
      "Train Epoch: 17 [44000/60000] Loss: 1.0196171998977661\n",
      "Train Epoch: 17 [45000/60000] Loss: 1.13449227809906\n",
      "Train Epoch: 17 [46000/60000] Loss: 1.0559066534042358\n",
      "Train Epoch: 17 [47000/60000] Loss: 0.9910051226615906\n",
      "Train Epoch: 17 [48000/60000] Loss: 1.0628066062927246\n",
      "Train Epoch: 17 [49000/60000] Loss: 0.8050541877746582\n",
      "Train Epoch: 17 [50000/60000] Loss: 0.9187374711036682\n",
      "Train Epoch: 17 [51000/60000] Loss: 0.906877875328064\n",
      "Train Epoch: 17 [52000/60000] Loss: 1.0287224054336548\n",
      "Train Epoch: 17 [53000/60000] Loss: 1.1308106184005737\n",
      "Train Epoch: 17 [54000/60000] Loss: 0.7504885196685791\n",
      "Train Epoch: 17 [55000/60000] Loss: 0.7948195934295654\n",
      "Train Epoch: 17 [56000/60000] Loss: 0.8392407894134521\n",
      "Train Epoch: 17 [57000/60000] Loss: 0.7750658392906189\n",
      "Train Epoch: 17 [58000/60000] Loss: 1.0340166091918945\n",
      "Train Epoch: 17 [59000/60000] Loss: 0.924343466758728\n",
      "\n",
      "Test set: Average loss: 0.0059794874966144565, Accuracy: 8573/10000 (85.73)\n",
      "\n",
      "Train Epoch: 18 [0/60000] Loss: 0.8834887742996216\n",
      "Train Epoch: 18 [1000/60000] Loss: 0.8696773648262024\n",
      "Train Epoch: 18 [2000/60000] Loss: 0.8401680588722229\n",
      "Train Epoch: 18 [3000/60000] Loss: 1.0749804973602295\n",
      "Train Epoch: 18 [4000/60000] Loss: 1.0279130935668945\n",
      "Train Epoch: 18 [5000/60000] Loss: 1.1880604028701782\n",
      "Train Epoch: 18 [6000/60000] Loss: 0.9149755835533142\n",
      "Train Epoch: 18 [7000/60000] Loss: 1.0161901712417603\n",
      "Train Epoch: 18 [8000/60000] Loss: 1.0226212739944458\n",
      "Train Epoch: 18 [9000/60000] Loss: 0.8867511749267578\n",
      "Train Epoch: 18 [10000/60000] Loss: 0.9576377272605896\n",
      "Train Epoch: 18 [11000/60000] Loss: 0.9397438764572144\n",
      "Train Epoch: 18 [12000/60000] Loss: 0.8835929036140442\n",
      "Train Epoch: 18 [13000/60000] Loss: 0.8029626607894897\n",
      "Train Epoch: 18 [14000/60000] Loss: 1.0731570720672607\n",
      "Train Epoch: 18 [15000/60000] Loss: 1.1809933185577393\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.95965576171875\n",
      "Train Epoch: 18 [17000/60000] Loss: 0.9367177486419678\n",
      "Train Epoch: 18 [18000/60000] Loss: 1.0305262804031372\n",
      "Train Epoch: 18 [19000/60000] Loss: 0.9740092754364014\n",
      "Train Epoch: 18 [20000/60000] Loss: 0.9877204895019531\n",
      "Train Epoch: 18 [21000/60000] Loss: 0.8882110118865967\n",
      "Train Epoch: 18 [22000/60000] Loss: 0.9513817429542542\n",
      "Train Epoch: 18 [23000/60000] Loss: 0.8903730511665344\n",
      "Train Epoch: 18 [24000/60000] Loss: 1.0025973320007324\n",
      "Train Epoch: 18 [25000/60000] Loss: 0.9804943799972534\n",
      "Train Epoch: 18 [26000/60000] Loss: 0.9597731828689575\n",
      "Train Epoch: 18 [27000/60000] Loss: 1.1256405115127563\n",
      "Train Epoch: 18 [28000/60000] Loss: 0.984548032283783\n",
      "Train Epoch: 18 [29000/60000] Loss: 0.9281120300292969\n",
      "Train Epoch: 18 [30000/60000] Loss: 0.8450663089752197\n",
      "Train Epoch: 18 [31000/60000] Loss: 0.8115473985671997\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.9710685610771179\n",
      "Train Epoch: 18 [33000/60000] Loss: 0.8517956733703613\n",
      "Train Epoch: 18 [34000/60000] Loss: 0.9871410131454468\n",
      "Train Epoch: 18 [35000/60000] Loss: 0.9532884955406189\n",
      "Train Epoch: 18 [36000/60000] Loss: 1.1176104545593262\n",
      "Train Epoch: 18 [37000/60000] Loss: 0.9575042128562927\n",
      "Train Epoch: 18 [38000/60000] Loss: 0.8453284502029419\n",
      "Train Epoch: 18 [39000/60000] Loss: 1.1094329357147217\n",
      "Train Epoch: 18 [40000/60000] Loss: 0.8824607729911804\n",
      "Train Epoch: 18 [41000/60000] Loss: 0.8816735148429871\n",
      "Train Epoch: 18 [42000/60000] Loss: 0.8685983419418335\n",
      "Train Epoch: 18 [43000/60000] Loss: 0.7788783311843872\n",
      "Train Epoch: 18 [44000/60000] Loss: 0.9385057687759399\n",
      "Train Epoch: 18 [45000/60000] Loss: 0.8645275235176086\n",
      "Train Epoch: 18 [46000/60000] Loss: 1.0366426706314087\n",
      "Train Epoch: 18 [47000/60000] Loss: 0.8985660672187805\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.928748607635498\n",
      "Train Epoch: 18 [49000/60000] Loss: 0.9078270792961121\n",
      "Train Epoch: 18 [50000/60000] Loss: 0.8751634359359741\n",
      "Train Epoch: 18 [51000/60000] Loss: 1.0430556535720825\n",
      "Train Epoch: 18 [52000/60000] Loss: 0.8889331221580505\n",
      "Train Epoch: 18 [53000/60000] Loss: 0.8448641300201416\n",
      "Train Epoch: 18 [54000/60000] Loss: 0.6883458495140076\n",
      "Train Epoch: 18 [55000/60000] Loss: 0.8562567234039307\n",
      "Train Epoch: 18 [56000/60000] Loss: 0.9283196330070496\n",
      "Train Epoch: 18 [57000/60000] Loss: 0.8636772036552429\n",
      "Train Epoch: 18 [58000/60000] Loss: 1.0462020635604858\n",
      "Train Epoch: 18 [59000/60000] Loss: 1.0719892978668213\n",
      "\n",
      "Test set: Average loss: 0.005416766396164894, Accuracy: 8662/10000 (86.62)\n",
      "\n",
      "Train Epoch: 19 [0/60000] Loss: 0.7933589816093445\n",
      "Train Epoch: 19 [1000/60000] Loss: 0.9012303948402405\n",
      "Train Epoch: 19 [2000/60000] Loss: 0.9105309844017029\n",
      "Train Epoch: 19 [3000/60000] Loss: 0.8940053582191467\n",
      "Train Epoch: 19 [4000/60000] Loss: 1.011182427406311\n",
      "Train Epoch: 19 [5000/60000] Loss: 0.8140820264816284\n",
      "Train Epoch: 19 [6000/60000] Loss: 0.8648414015769958\n",
      "Train Epoch: 19 [7000/60000] Loss: 0.993416965007782\n",
      "Train Epoch: 19 [8000/60000] Loss: 0.8472342491149902\n",
      "Train Epoch: 19 [9000/60000] Loss: 0.7651407122612\n",
      "Train Epoch: 19 [10000/60000] Loss: 0.8281848430633545\n",
      "Train Epoch: 19 [11000/60000] Loss: 0.8543246388435364\n",
      "Train Epoch: 19 [12000/60000] Loss: 0.9712945818901062\n",
      "Train Epoch: 19 [13000/60000] Loss: 0.9277466535568237\n",
      "Train Epoch: 19 [14000/60000] Loss: 0.8614855408668518\n",
      "Train Epoch: 19 [15000/60000] Loss: 0.9002315402030945\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.9854192137718201\n",
      "Train Epoch: 19 [17000/60000] Loss: 1.0934226512908936\n",
      "Train Epoch: 19 [18000/60000] Loss: 1.0082226991653442\n",
      "Train Epoch: 19 [19000/60000] Loss: 0.9043352603912354\n",
      "Train Epoch: 19 [20000/60000] Loss: 0.9502319097518921\n",
      "Train Epoch: 19 [21000/60000] Loss: 0.8951534032821655\n",
      "Train Epoch: 19 [22000/60000] Loss: 0.6770120859146118\n",
      "Train Epoch: 19 [23000/60000] Loss: 0.9611913561820984\n",
      "Train Epoch: 19 [24000/60000] Loss: 0.8986126780509949\n",
      "Train Epoch: 19 [25000/60000] Loss: 0.9172195196151733\n",
      "Train Epoch: 19 [26000/60000] Loss: 0.743258535861969\n",
      "Train Epoch: 19 [27000/60000] Loss: 1.0775421857833862\n",
      "Train Epoch: 19 [28000/60000] Loss: 0.8867301344871521\n",
      "Train Epoch: 19 [29000/60000] Loss: 0.8467191457748413\n",
      "Train Epoch: 19 [30000/60000] Loss: 0.963875949382782\n",
      "Train Epoch: 19 [31000/60000] Loss: 0.9051334857940674\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.8485499024391174\n",
      "Train Epoch: 19 [33000/60000] Loss: 0.9988102912902832\n",
      "Train Epoch: 19 [34000/60000] Loss: 0.8644156455993652\n",
      "Train Epoch: 19 [35000/60000] Loss: 0.8872418999671936\n",
      "Train Epoch: 19 [36000/60000] Loss: 0.9463933706283569\n",
      "Train Epoch: 19 [37000/60000] Loss: 0.89885413646698\n",
      "Train Epoch: 19 [38000/60000] Loss: 0.8842618465423584\n",
      "Train Epoch: 19 [39000/60000] Loss: 0.9674968123435974\n",
      "Train Epoch: 19 [40000/60000] Loss: 0.8721444606781006\n",
      "Train Epoch: 19 [41000/60000] Loss: 0.9083794951438904\n",
      "Train Epoch: 19 [42000/60000] Loss: 0.9946706891059875\n",
      "Train Epoch: 19 [43000/60000] Loss: 0.7899327874183655\n",
      "Train Epoch: 19 [44000/60000] Loss: 0.968213677406311\n",
      "Train Epoch: 19 [45000/60000] Loss: 0.9250995516777039\n",
      "Train Epoch: 19 [46000/60000] Loss: 1.1171488761901855\n",
      "Train Epoch: 19 [47000/60000] Loss: 0.840033769607544\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.9215796589851379\n",
      "Train Epoch: 19 [49000/60000] Loss: 0.611482560634613\n",
      "Train Epoch: 19 [50000/60000] Loss: 0.8318967223167419\n",
      "Train Epoch: 19 [51000/60000] Loss: 0.814259946346283\n",
      "Train Epoch: 19 [52000/60000] Loss: 0.9081963896751404\n",
      "Train Epoch: 19 [53000/60000] Loss: 0.9478318691253662\n",
      "Train Epoch: 19 [54000/60000] Loss: 0.8257148265838623\n",
      "Train Epoch: 19 [55000/60000] Loss: 0.7907350063323975\n",
      "Train Epoch: 19 [56000/60000] Loss: 0.9213071465492249\n",
      "Train Epoch: 19 [57000/60000] Loss: 0.6976264119148254\n",
      "Train Epoch: 19 [58000/60000] Loss: 0.8662667274475098\n",
      "Train Epoch: 19 [59000/60000] Loss: 0.8558646440505981\n",
      "\n",
      "Test set: Average loss: 0.004948864045739174, Accuracy: 8749/10000 (87.49)\n",
      "\n",
      "Train Epoch: 20 [0/60000] Loss: 0.825991690158844\n",
      "Train Epoch: 20 [1000/60000] Loss: 0.8511020541191101\n",
      "Train Epoch: 20 [2000/60000] Loss: 0.9153515696525574\n",
      "Train Epoch: 20 [3000/60000] Loss: 0.8750317096710205\n",
      "Train Epoch: 20 [4000/60000] Loss: 0.8310208320617676\n",
      "Train Epoch: 20 [5000/60000] Loss: 0.7745293974876404\n",
      "Train Epoch: 20 [6000/60000] Loss: 0.8185027241706848\n",
      "Train Epoch: 20 [7000/60000] Loss: 0.7971163392066956\n",
      "Train Epoch: 20 [8000/60000] Loss: 0.9523584842681885\n",
      "Train Epoch: 20 [9000/60000] Loss: 0.8750778436660767\n",
      "Train Epoch: 20 [10000/60000] Loss: 1.2398658990859985\n",
      "Train Epoch: 20 [11000/60000] Loss: 0.8301780223846436\n",
      "Train Epoch: 20 [12000/60000] Loss: 0.749032735824585\n",
      "Train Epoch: 20 [13000/60000] Loss: 0.7137256860733032\n",
      "Train Epoch: 20 [14000/60000] Loss: 0.8317075967788696\n",
      "Train Epoch: 20 [15000/60000] Loss: 0.9041633009910583\n",
      "Train Epoch: 20 [16000/60000] Loss: 1.0015736818313599\n",
      "Train Epoch: 20 [17000/60000] Loss: 0.8200310468673706\n",
      "Train Epoch: 20 [18000/60000] Loss: 0.6890294551849365\n",
      "Train Epoch: 20 [19000/60000] Loss: 0.7677714824676514\n",
      "Train Epoch: 20 [20000/60000] Loss: 0.7349360585212708\n",
      "Train Epoch: 20 [21000/60000] Loss: 0.7523318529129028\n",
      "Train Epoch: 20 [22000/60000] Loss: 0.7557939291000366\n",
      "Train Epoch: 20 [23000/60000] Loss: 0.8662159442901611\n",
      "Train Epoch: 20 [24000/60000] Loss: 0.8325784206390381\n",
      "Train Epoch: 20 [25000/60000] Loss: 0.9884801506996155\n",
      "Train Epoch: 20 [26000/60000] Loss: 0.9896610975265503\n",
      "Train Epoch: 20 [27000/60000] Loss: 0.7498443722724915\n",
      "Train Epoch: 20 [28000/60000] Loss: 0.8311048746109009\n",
      "Train Epoch: 20 [29000/60000] Loss: 0.7927848100662231\n",
      "Train Epoch: 20 [30000/60000] Loss: 0.9463869333267212\n",
      "Train Epoch: 20 [31000/60000] Loss: 0.8337305188179016\n",
      "Train Epoch: 20 [32000/60000] Loss: 0.8032544851303101\n",
      "Train Epoch: 20 [33000/60000] Loss: 1.0606248378753662\n",
      "Train Epoch: 20 [34000/60000] Loss: 0.8353840708732605\n",
      "Train Epoch: 20 [35000/60000] Loss: 0.7602024078369141\n",
      "Train Epoch: 20 [36000/60000] Loss: 0.9322474002838135\n",
      "Train Epoch: 20 [37000/60000] Loss: 0.7171195149421692\n",
      "Train Epoch: 20 [38000/60000] Loss: 1.0067250728607178\n",
      "Train Epoch: 20 [39000/60000] Loss: 0.7847186923027039\n",
      "Train Epoch: 20 [40000/60000] Loss: 0.7954081892967224\n",
      "Train Epoch: 20 [41000/60000] Loss: 0.8203674554824829\n",
      "Train Epoch: 20 [42000/60000] Loss: 0.9281942844390869\n",
      "Train Epoch: 20 [43000/60000] Loss: 0.7689693570137024\n",
      "Train Epoch: 20 [44000/60000] Loss: 0.9080271124839783\n",
      "Train Epoch: 20 [45000/60000] Loss: 0.746995210647583\n",
      "Train Epoch: 20 [46000/60000] Loss: 0.716976523399353\n",
      "Train Epoch: 20 [47000/60000] Loss: 0.6468859910964966\n",
      "Train Epoch: 20 [48000/60000] Loss: 0.8159489631652832\n",
      "Train Epoch: 20 [49000/60000] Loss: 0.9647253155708313\n",
      "Train Epoch: 20 [50000/60000] Loss: 0.7724634408950806\n",
      "Train Epoch: 20 [51000/60000] Loss: 0.8941550254821777\n",
      "Train Epoch: 20 [52000/60000] Loss: 0.7666618227958679\n",
      "Train Epoch: 20 [53000/60000] Loss: 0.8477275967597961\n",
      "Train Epoch: 20 [54000/60000] Loss: 0.7662767767906189\n",
      "Train Epoch: 20 [55000/60000] Loss: 0.8531820774078369\n",
      "Train Epoch: 20 [56000/60000] Loss: 0.8623229265213013\n",
      "Train Epoch: 20 [57000/60000] Loss: 0.8829092383384705\n",
      "Train Epoch: 20 [58000/60000] Loss: 0.7722355723381042\n",
      "Train Epoch: 20 [59000/60000] Loss: 0.7488367557525635\n",
      "\n",
      "Test set: Average loss: 0.004590472906827927, Accuracy: 8833/10000 (88.33)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 21):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prdiction pour l'image 1: 6\n",
      "Valeur relle: 6\n",
      "Prdiction pour l'image 2: 0\n",
      "Valeur relle: 0\n",
      "Prdiction pour l'image 3: 5\n",
      "Valeur relle: 5\n",
      "Prdiction pour l'image 4: 4\n",
      "Valeur relle: 4\n",
      "Prdiction pour l'image 5: 9\n",
      "Valeur relle: 9\n",
      "Prdiction pour l'image 6: 9\n",
      "Valeur relle: 9\n",
      "Prdiction pour l'image 7: 2\n",
      "Valeur relle: 2\n",
      "Prdiction pour l'image 8: 1\n",
      "Valeur relle: 1\n",
      "Prdiction pour l'image 9: 9\n",
      "Valeur relle: 9\n",
      "Prdiction pour l'image 10: 4\n",
      "Valeur relle: 4\n",
      "\n",
      "\n",
      "Note: 10/10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leModel = Net()\n",
    "\n",
    "leModel.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "\n",
    "leModel.eval()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    data, target = test_data[i+100]\n",
    "    data = data.unsqueeze(0).to(device)\n",
    "\n",
    "    output = leModel(data)\n",
    "\n",
    "    prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "    print(f\"Prdiction pour l'image {i+1}: {prediction}\")\n",
    "\n",
    "    image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(image, cmap=\"rainbow\")\n",
    "\n",
    "    print(f\"Valeur relle: {target}\")\n",
    "\n",
    "good_predictions = 0\n",
    "\n",
    "for i in range(10):\n",
    "    data, target = test_data[i+100]\n",
    "    data = data.unsqueeze(0).to(device)\n",
    "    output = leModel(data)\n",
    "    prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "    image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    if prediction == target:\n",
    "        good_predictions += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(f\"Note: {good_predictions}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
